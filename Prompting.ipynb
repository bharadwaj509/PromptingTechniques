{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2595d371-9a27-4c19-9b46-c786b17479af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Initialize the OpenAI client with your API key\n",
    "client = OpenAI(api_key=\"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\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e51fc28-228b-43e2-8740-9e8894604694",
   "metadata": {},
   "source": [
    "## 1. Chain of Thought (CoT) prompting \n",
    "### It is an advanced technique used in AI language models to enhance their reasoning capabilities by explicitly encouraging step-by-step logical thinking. It is particularly effective for solving problems that require multi-step reasoning, such as mathematical calculations, logical puzzles, or complex decision-making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec691a1c-da02-455c-bbe5-65468e22f3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain-of-Thought Reasoning:\n",
      "\n",
      "Here's the solution:\n",
      "\n",
      "Step 1: Calculate the original profit per month.\n",
      "Profit per unit = selling price per unit - production cost per unit = $50 - $30 = $20.\n",
      "So, the total monthly profit = profit per unit * number of units sold per month = $20 * 200 = $4000.\n",
      "\n",
      "Step 2: Calculate the new expected sales volume.\n",
      "If the sales drop by 20%, the new sales volume = 200 - 20% * 200 = 200 - 40 = 160 units.\n",
      "\n",
      "Step 3: Calculate the new profit per unit.\n",
      "The new profit per unit = new selling price per unit - production cost per unit = $60 - $30 = $30.\n",
      "\n",
      "Step 4: Calculate the new monthly profit.\n",
      "The new monthly profit = new profit per unit * new number of units sold per month = $30 * 160 = $4800.\n",
      "\n",
      "So, if the company increases the price to $60 per unit and expects sales to drop by 20%, the new profit per month will be $4800.\n"
     ]
    }
   ],
   "source": [
    "# Chain of thought printint step by step\n",
    "\n",
    "# Define the Chain-of-Thought reasoning prompt\n",
    "cot_prompt = \"\"\"\n",
    "You are a mathematician. Solve this problem using step-by-step reasoning:\n",
    "\n",
    "Question: A company sells a product at $50 per unit. The production cost per unit is $30, and they sell 200 units per month. \n",
    "If the company plans to increase the price to $60 per unit and expects sales to drop by 20%, what will be the new profit per month?\n",
    "\"\"\"\n",
    "\n",
    "def generate_response():\n",
    "    # Generate the complete response (non-streaming)\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert in mathematics and financial analysis.\"},\n",
    "            {\"role\": \"user\", \"content\": cot_prompt}\n",
    "        ],\n",
    "        stream=False  # Non-streaming mode\n",
    "    )\n",
    "\n",
    "    # Extract and display the content\n",
    "    content = response.choices[0].message.content\n",
    "    print(\"Chain-of-Thought Reasoning:\\n\")\n",
    "    print(content)\n",
    "\n",
    "# Generate and display the response\n",
    "generate_response()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6906eb02-e80c-498c-811b-3fa318a76419",
   "metadata": {},
   "source": [
    "### Without CoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9186805e-af68-4a3d-95d7-3840acc8cbf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Answer:\n",
      "\n",
      "$5280\n"
     ]
    }
   ],
   "source": [
    "# Define the prompt asking for the final answer only\n",
    "direct_answer_prompt = \"\"\"\n",
    "You are a mathematician. Provide only the final answer, skipping all intermediate steps.\n",
    "\n",
    "Question: A company sells a product at $50 per unit. The production cost per unit is $30, and they sell 200 units per month. \n",
    "If the company plans to increase the price to $60 per unit and expects sales to drop by 20%, what will be the new profit per month?\n",
    "\"\"\"\n",
    "\n",
    "def generate_response():\n",
    "    # Generate the response\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert in mathematics and financial analysis.\"},\n",
    "            {\"role\": \"user\", \"content\": direct_answer_prompt}\n",
    "        ],\n",
    "        stream=False\n",
    "    )\n",
    "\n",
    "    # Extract and display the content\n",
    "    content = response.choices[0].message.content\n",
    "    print(\"Final Answer:\\n\")\n",
    "    print(content)\n",
    "\n",
    "# Generate and display the response\n",
    "generate_response()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84aafad2-b16a-4544-802c-867e1643d03b",
   "metadata": {},
   "source": [
    "## 2. Few-Shot Prompting\n",
    "### Concept: Provide several examples of input-output pairs before the actual task. This technique helps the model generalize from examples.\n",
    "### Use Case: When exact behavior tuning is needed, such as text classification or translation tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a88bf08-a0d8-497c-bc5b-53671c08cc5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debugging Solution:\n",
      "\n",
      "**Error Analysis**: The function `add_numbers(a, b)` is defined to take two arguments. However, when the function is called in the print statement only one argument is provided. Consequently, Python raises an error because it's expecting two arguments but only receives one.\n",
      "\n",
      "**Fixed Code**:\n",
      "```python\n",
      "def add_numbers(a, b=0): # a default value to the second parameter\n",
      "    return a + b\n",
      "print(add_numbers(10)) \n",
      "```\n",
      "In the above fixed code, if a second parameter is not provided, Python will use a default value of `0` for `b`. Therefore, calling `add_numbers(10)` is equivalent to calling `add_numbers(10, 0)`.\n"
     ]
    }
   ],
   "source": [
    "# Define the few-shot prompt for debugging Python code\n",
    "few_shot_prompt = \"\"\"\n",
    "You are a Python expert skilled at debugging code. Given a problem, analyze the error, explain the likely cause, and provide a fixed version of the code.\n",
    "\n",
    "Examples:\n",
    "1. **Problem**:\n",
    "   ```python\n",
    "   def add_numbers(a, b):\n",
    "       return a + b\n",
    "   print(add_numbers(10))\n",
    "\"\"\"\n",
    "\n",
    "def generate_response(): # Generate a response using the few-shot prompt \n",
    "    response = client.chat.completions.create( \n",
    "        model=\"gpt-4\", \n",
    "        messages=[ \n",
    "            {\"role\": \"system\", \n",
    "             \"content\": \"You are a Python debugging assistant.\"}, \n",
    "            {\"role\": \"user\", \"content\": few_shot_prompt}, ], \n",
    "        stream=False # Non-streaming mode \n",
    "    )\n",
    "    # Extract and display the content\n",
    "    content = response.choices[0].message.content\n",
    "    print(\"Debugging Solution:\\n\")\n",
    "    print(content)\n",
    "generate_response()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4536a30-7fba-48ec-a59c-ed708ef9a342",
   "metadata": {},
   "source": [
    "## 3. Dynamic Variable Substitution\n",
    "### Concept: Use placeholders or programmatically inject context-relevant variables into the prompt.\n",
    "### Use Case: Personalized responses, database-backed applications, or dynamically generated prompts in real-time systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5b54795-7fc3-4c7e-aaf2-888f86fc1317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Generated Python API Wrapper ###\n",
      "\n",
      "Here's the Python API wrapper class for the given RESTful API specification:\n",
      "\n",
      "```python\n",
      "import requests\n",
      "from requests.exceptions import RequestException\n",
      "\n",
      "class RestAPIWrapper:\n",
      "    BASE_URL = 'https://api.example.com'\n",
      "    \n",
      "    def __init__(self, token):\n",
      "        self.headers = {\n",
      "            'Authorization': f'Bearer {token}',\n",
      "            'Content-Type': 'application/json'\n",
      "        }\n",
      "\n",
      "    def get_user(self, user_id):\n",
      "        url = f'{self.BASE_URL}/users/{user_id}'\n",
      "        try:\n",
      "            response = requests.get(url, headers=self.headers)\n",
      "            response.raise_for_status() \n",
      "            return response.json()\n",
      "        except RequestException as e:\n",
      "            print(f'Request failed: {str(e)}')\n",
      "\n",
      "    def create_user(self, name, email):\n",
      "        url = f'{self.BASE_URL}/users'\n",
      "        payload = { 'name': name, 'email': email }\n",
      "        try:\n",
      "            response = requests.post(url, headers=self.headers, json=payload)\n",
      "            response.raise_for_status()\n",
      "            return response.json()\n",
      "        except RequestException as e:\n",
      "            print(f'Request failed: {str(e)}')\n",
      "\n",
      "    def delete_user(self, user_id):\n",
      "        url = f'{self.BASE_URL}/users/{user_id}'\n",
      "        try:\n",
      "            response = requests.delete(url, headers=self.headers)\n",
      "            response.raise_for_status()\n",
      "            return response.json()\n",
      "        except RequestException as e:\n",
      "            print(f'Request failed: {str(e)}')\n",
      "```\n",
      "\n",
      "To initialize this wrapper and use it to create, get, and delete users:\n",
      "```python\n",
      "api = RestAPIWrapper('<your token>')\n",
      "api.create_user('John Doe', 'johndoe@example.com')\n",
      "user = api.get_user('<user id>')\n",
      "api.delete_user('<user id>')\n",
      "```\n",
      "\n",
      "Please replace '<your token>' and '<user id>' with actual values in your case. Also, please note that while this code handles HTTP errors, you may need to add custom error handling based on the API's specific error responses.\n",
      "\n",
      "This wrapper class will create, get, and delete user data from the API. The class uses the requests library to send HTTP requests to the given endpoints, and includes error handling for failed requests. The Authorization header is set using your Bearer token.\n"
     ]
    }
   ],
   "source": [
    "# Dynamic variables to be substituted into the prompt\n",
    "api_spec = {\n",
    "    \"base_url\": \"https://api.example.com\",\n",
    "    \"endpoints\": [\n",
    "        {\"name\": \"get_user\", \"path\": \"/users/{user_id}\", \"method\": \"GET\"},\n",
    "        {\"name\": \"create_user\", \"path\": \"/users\", \"method\": \"POST\", \"payload\": {\"name\": \"string\", \"email\": \"string\"}},\n",
    "        {\"name\": \"delete_user\", \"path\": \"/users/{user_id}\", \"method\": \"DELETE\"},\n",
    "    ],\n",
    "    \"authentication_type\": \"Bearer Token\",\n",
    "}\n",
    "\n",
    "# Dynamic prompt generation\n",
    "dynamic_prompt_template = \"\"\"\n",
    "You are a Python expert. Generate a Python API wrapper class for the following RESTful API specification:\n",
    "\n",
    "Base URL: {base_url}\n",
    "\n",
    "Endpoints:\n",
    "{endpoints}\n",
    "\n",
    "Authentication: {authentication_type}\n",
    "\n",
    "The wrapper should:\n",
    "- Include methods for each endpoint (e.g., `get_user`, `create_user`, `delete_user`).\n",
    "- Use the `requests` library for HTTP requests.\n",
    "- Handle authentication using the specified type.\n",
    "- Include appropriate error handling.\n",
    "\n",
    "Example request for `create_user`:\n",
    "- Endpoint: POST {base_url}/users\n",
    "- Payload: {{ \"name\": \"string\", \"email\": \"string\" }}\n",
    "\n",
    "Output the complete Python code.\n",
    "\"\"\"\n",
    "\n",
    "# Format the endpoints for the dynamic prompt\n",
    "formatted_endpoints = \"\\n\".join(\n",
    "    f\"- {ep['name']}: {ep['method']} {api_spec['base_url']}{ep['path']}\" for ep in api_spec[\"endpoints\"]\n",
    ")\n",
    "\n",
    "# Generate the final prompt with substituted variables\n",
    "final_prompt = dynamic_prompt_template.format(\n",
    "    base_url=api_spec[\"base_url\"],\n",
    "    endpoints=formatted_endpoints,\n",
    "    authentication_type=api_spec[\"authentication_type\"],\n",
    ")\n",
    "\n",
    "def generate_api_wrapper():\n",
    "    # Generate the API wrapper code using OpenAI's GPT\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert in Python API development.\"},\n",
    "            {\"role\": \"user\", \"content\": final_prompt},\n",
    "        ],\n",
    "        stream=False\n",
    "    )\n",
    "\n",
    "    # Extract and print the generated API wrapper\n",
    "    generated_code = response.choices[0].message.content\n",
    "    print(\"\\n### Generated Python API Wrapper ###\\n\")\n",
    "    print(generated_code)\n",
    "\n",
    "# Run the function\n",
    "generate_api_wrapper()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd68eca-c8d4-46eb-807f-9747f4692f38",
   "metadata": {},
   "source": [
    "## 4. Multimodal Prompting\n",
    "### Concept: Combine inputs from multiple modalities (e.g., text and images) for richer responses.\n",
    "### Use Case: Cross-modal reasoning, advanced analytics, or multimodal creativity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f6ce713-26ae-414a-8e5f-2605bc692964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Data Analysis ###\n",
      "\n",
      "Analysis:\n",
      "\n",
      "1. **North Region:** This region experienced significant sales fluctuations throughout the year. The highest sales numbers happened in March (50k) and November (45k), indicating that these months might be the best for product launches or marketing campaigns. Conversely, there is a substantial dip in sales in August (20k). \n",
      "\n",
      "2. **South Region:** This area exhibited consistent growth in sales performance from January (10k) to December (40k). This steady increase could demonstrate effective sales strategies that are progressively attracting more customers.\n",
      "\n",
      "3. **East Region:** East experienced a noteworthy drop in sales in May (from 30k to 5k) but quickly recovered in June (35k). This drop could indicate a one-time issue that negatively impacted sales, such as supply chain disruptions or marketing failures.\n",
      "\n",
      "Recommendations:\n",
      "\n",
      "1. **North Region:** Investigate what leads to high sales in March and November. Are there specific events, promotions, or seasonal factors affecting these peaks? Furthermore, figure out the reasons for the dip in August. If it's due to external factors such as vacation season, it could be a good time to reduce inventory or cut costs.\n",
      "\n",
      "2. **South Region:** Highlight and replicate the strategies employed in this region, where we observe steady growth throughout the year. Identify what causes this consistent increase, such as successful marketing campaigns, and apply this knowledge to other regions.\n",
      "\n",
      "3. **East Region:** Examine the reason for the steep decline in May. If the issue is internal like supply chain disruptions, corrective action needs to be implemented. If the reason is external, such as an unexpected event, it would be essential to develop risk mitigation strategies.\n",
      "\n",
      "Overall, the company should continue monitoring sales performance closely and adjusting strategies according to the observed trends and challenges. A more granular analysis on a weekly or daily basis may also provide additional insights.\n"
     ]
    }
   ],
   "source": [
    "# Example text description about a dataset\n",
    "text_description = \"\"\"\n",
    "I have a dataset tracking monthly sales performance across three regions: North, South, and East. \n",
    "The attached figure is a bar chart showing monthly sales for the year 2023.\n",
    "I need help identifying trends, outliers, and any actionable insights.\n",
    "\"\"\"\n",
    "\n",
    "# Simulated OCR or captioning output from a bar chart image\n",
    "image_to_text = \"\"\"\n",
    "Graph Analysis:\n",
    "- Title: Monthly Sales Performance (2023)\n",
    "- X-axis: Months (January to December)\n",
    "- Y-axis: Sales (in USD, thousands)\n",
    "- Data:\n",
    "    - North: Peaks in March (50k) and November (45k); dip in August (20k).\n",
    "    - South: Steady growth from January (10k) to December (40k).\n",
    "    - East: Sharp drop in May (30k to 5k) with recovery in June (35k).\n",
    "\"\"\"\n",
    "\n",
    "# Combine the text description and \"image-to-text\" into a multimodal prompt\n",
    "multimodal_prompt = f\"\"\"\n",
    "You are a data analysis expert. Below is a description of a dataset and a simulated text representation of a graph. \n",
    "Analyze the data, identify key trends, and provide actionable insights.\n",
    "\n",
    "Text Description:\n",
    "{text_description}\n",
    "\n",
    "Graph (Simulated Text Form):\n",
    "{image_to_text}\n",
    "\n",
    "Provide your analysis and any recommendations for the business based on the data trends.\n",
    "\"\"\"\n",
    "\n",
    "def analyze_graph():\n",
    "    # Generate the analysis using OpenAI's API\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a data analysis assistant skilled in identifying insights from graphs and data.\"},\n",
    "            {\"role\": \"user\", \"content\": multimodal_prompt},\n",
    "        ],\n",
    "        stream=False  # Non-streaming mode\n",
    "    )\n",
    "\n",
    "    # Extract and print the analysis\n",
    "    analysis = response.choices[0].message.content\n",
    "    print(\"\\n### Data Analysis ###\\n\")\n",
    "    print(analysis)\n",
    "\n",
    "# Run the function\n",
    "analyze_graph()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804dad4c-ed65-4d5d-957a-f0295b4ee581",
   "metadata": {},
   "source": [
    "### 5. Roleplay Contextualization\n",
    "### Concept: Assign the AI a specific role, which guides its perspective and tone.\n",
    "### Use Case: Deeply domain-specific explanations, teaching, or simulating expert opinions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eaa0d483-0e72-4e81-a2c6-57439ab6dd26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Cloud Architecture Review ###\n",
      "\n",
      "**Detailed Evaluation**:\n",
      "\n",
      "The current architecture has made good use of some core services offered by AWS like EC2, S3, RDS and CloudWatch. However, given the constraint of a 5x growth in traffic and grace handling of regional outages, the architecture lacks on few points:\n",
      "\n",
      "- Absence of Auto Scaling in the architecture will have to deal with sudden surge in traffic. \n",
      "- Hosting everything in single region does not provide fault tolerance in case of a regional outage.\n",
      "- Proper cost management technique isn't outlined like Reserved Instances, Savings Plans, right sizing etc. \n",
      "\n",
      "**Recommendations**:\n",
      "\n",
      "1. **Scalability** :\n",
      "    - Implement Auto Scaling Groups with EC2 instances for the frontend layer. This will ensure that during peak load times, new instances are spun up automatically to handle the load spike. \n",
      "    - For Amazon RDS, you can enable vertical and horizontal scaling. Vertical by increasing the instance size (in terms of CPU and memory) and horizontal by using Read Replicas. \n",
      "    - Introduce a Content Delivery Network service like Amazon CloudFront to serve static assets, which will reduce the load on S3 and provide a faster response time.\n",
      "2. **Fault Tolerence** :\n",
      "    - Distribute the application across multiple AWS regions or at least multi-Availability Zones (AZs) within the region. Use Route53 (Amazon’s DNS service) to route the traffic to different regions/AZs in an active-active or active-standby architecture based on the actual requirement.\n",
      "    - Make sure that RDS instances are Multi-AZ to have automatic failover support.\n",
      "    - Consider leveraging Amazon S3's Cross Region Replication feature to ensure the high availability and durability of the static assets.\n",
      "3. **Cost-Efficiency** :\n",
      "    - Utilizing Auto Scaling not just improves availability and scalability, but you pay for only what you use which improves cost-efficiency.\n",
      "    - For predictable or steady state workloads, use Reserved Instances and/or Savings Plans which enables significant savings (up to 72%) compared to On-demand price.\n",
      "    - Make use of caching layers like Amazon ElastiCache to cache frequent DB queries and reduce the read load on RDS, thus saving costs.\n",
      "    - Move non-critical, infrequently accessed data to Amazon S3 Standard-IA or Glacier to reduce S3 costs.\n",
      "\n",
      "**Example Configurations**:\n",
      "\n",
      "- Auto Scaling can be configured to monitor the load on EC2 instances and to automatically scale up when the threshold is exceeded.\n",
      "- Amazon RDS instance used should be a Multi-AZ deployment. Also, creation of Read Replicas should be based on the requirement of the traffic.\n",
      "- A CloudFront distribution to be created with S3 bucket as the origin to serve static assets.\n",
      "- Route 53 can be used to distribute traffic among multi-regions/AZs.\n",
      "- Consider Reservation and saving plans for cost management. Regular cost optimisation review should be performed to identify possible optimisation areas to reduce cost.\n"
     ]
    }
   ],
   "source": [
    "# Roleplay Contextualization Prompt\n",
    "roleplay_prompt = \"\"\"\n",
    "You are a senior cloud architect at a leading tech company. Your task is to evaluate a proposed cloud architecture\n",
    "for a large-scale web application and suggest optimizations for scalability, fault tolerance, and cost-efficiency.\n",
    "\n",
    "**Proposed Cloud Architecture**:\n",
    "- A frontend layer hosted on AWS EC2 instances behind an Application Load Balancer (ALB).\n",
    "- A backend layer using an Amazon RDS (PostgreSQL) database.\n",
    "- Static assets (images, videos) served via Amazon S3.\n",
    "- Logging and monitoring handled by AWS CloudWatch.\n",
    "- No autoscaling is currently enabled.\n",
    "- All services are in a single AWS region (us-east-1).\n",
    "\n",
    "**Constraints**:\n",
    "1. Traffic is expected to grow by 5x in the next year.\n",
    "2. The system must handle regional outages gracefully.\n",
    "3. Budget increases should be justified by the improvements.\n",
    "\n",
    "Provide:\n",
    "1. A detailed evaluation of the architecture.\n",
    "2. Specific recommendations for improving scalability, fault tolerance, and cost-efficiency.\n",
    "3. Example configurations or technologies where applicable.\n",
    "\"\"\"\n",
    "\n",
    "def analyze_cloud_architecture():\n",
    "    # Generate response using OpenAI's GPT\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a senior cloud architect providing expert guidance on cloud solutions.\"},\n",
    "            {\"role\": \"user\", \"content\": roleplay_prompt},\n",
    "        ],\n",
    "        stream=False  # Non-streaming mode\n",
    "    )\n",
    "\n",
    "    # Extract and print the AI's role-played response\n",
    "    architecture_review = response.choices[0].message.content\n",
    "    print(\"\\n### Cloud Architecture Review ###\\n\")\n",
    "    print(architecture_review)\n",
    "\n",
    "# Run the function\n",
    "analyze_cloud_architecture()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776c9962-4f84-4466-8da7-298896a01908",
   "metadata": {},
   "source": [
    "## 6. Iterative Prompt Refinement\n",
    "### Concept: Build complex prompts iteratively by feeding the output of one prompt into another.\n",
    "### Use Case: Multi-stage workflows like content generation, data extraction, or creative writing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc6ea778-2359-4a00-9944-852609a07059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Iteration 1 ###\n",
      "\n",
      "Designing an optimal cloud-based data pipeline for ETL operations, considering the constraints, can be achieved by using the following technologies and architecture:\n",
      "\n",
      "1. **Data Ingestion**:\n",
      "   - **Kafka or AWS Kinesis**: For real-time data ingestion, Kafka or AWS Kinesis can consume data from sources like APIs.\n",
      "   - **S3 or Google Cloud Storage**: For static files like CSVs.\n",
      "   - **Change Data Capture (CDC)**: Data from databases can be ingested through CDC technology using tools like Debezium or Google's Datastream. They capture data changes and push it into a message queue like Kafka or Kinesis.\n",
      "\n",
      "2. **Data Processing/Transformation**:\n",
      "   - **Apache Beam or Spark Streaming**: They are excellent tools for processing real-time and batch data. You can perform all types of transformations including cleaning, aggregation, and enrichment.\n",
      "\n",
      "3. **Data Storage**: For intermediate storage, you can use:\n",
      "   - **Amazon S3 or Google Cloud Storage**: Cost-effective, high durability and availabilty make these services ideal for storing raw and intermediate data.\n",
      "\n",
      "4. **Data Warehouse**:\n",
      "   - **Snowflake or Google BigQuery**: You've specified Snowflake, and it's an excellent choice for the data warehouse. As an alternative, Google BigQuery also provides similar capabilities.\n",
      "\n",
      "5. **Workflow Management**:\n",
      "   - **Apache Airflow or Google Cloud Composer**: These tools will let you schedule and monitor workflows and can be used to manage dependencies among jobs.\n",
      "\n",
      "6. **Real-time Analytics**:\n",
      "   - **Elasticsearch or AWS Kinesis Data Analytics**: Both can provide real-time analytics capability to query real-time data.\n",
      "\n",
      "This whole pipeline can be deployed on the cloud (e.g., AWS or GCP) wherein we can ensure the scalability to handle 10x data growth via automatic scaling capabilities. Also, fault tolerance is inherent in their design.\n",
      "\n",
      "To reduce latency, data should be ingested and processed in real-time or near-real-time. In this case, Kafka Streams, Spark Streaming or Beam would be a good fit. \n",
      "\n",
      "For cost optimization, consider using spot/preemptible instances for non-critical, interruptible jobs. Similarly, auto-scaling and rightsizing infrastructure can also prevent overprovisioning and thereby save cost. Make sure to only persist necessary data to reduce storage costs. \n",
      "\n",
      "Remember, the specific technologies might still vary based on the specific requirements of the system and the programming languages and frameworks that the team is comfortable with.\n",
      "\n",
      "### Iteration 2 ###\n",
      "\n",
      "The following are some considerations and improvements for scalability, fault tolerance, cost efficiency and reducing bottlenecks:\n",
      "\n",
      "**Scalability:**\n",
      "1. **Elastic Load Balancer (ELB)/ Google Cloud Load Balancer**: It can help in load balancing which allows traffic to be distributed across multiple servers. ELB offers three types of load balancers that all feature high availability, automatic scaling, and robust security. This improves the availability and fault tolerance of applications.\n",
      "\n",
      "2. **Automated Scaling**: Both Kafka and Kinesis support automated scaling. These features should be setup correctly to handle uneven data loads. \n",
      "\n",
      "**Fault Tolerance:** \n",
      "1. **Data Replication**: For fault tolerance, you can set up data replication in Kafka and in the databases themselves. Make sure that the Kafka topics have enough replicas and that the database has a failover or standby instance ready in case the main one fails.\n",
      "\n",
      "2. **Self-healing (Kubernetes)**: Consider deploying your data pipeline components on Kubernetes. Kubernetes is excellent at managing and healing applications (like Apache Spark or Beam workers) that need to be highly available.\n",
      "\n",
      "**Cost Efficiency:**\n",
      "1. **Right Sizing**: Optimizing the size of your VMs and databases can help in cost reduction. Overprovisioning is often a cause of high costs, so regularly reassessing your resource needs can help keep costs down.\n",
      "\n",
      "2. **Use of Reserved Instances**: Cloud services often offer discounts for reserved instances which are used continuously compared to on-demand instances. This could be used for components which need to be always running.\n",
      "\n",
      "3. **Data Compression**: Compressing data before storing it in S3 or Google Cloud Storage can significantly reduce the amount of storage space used and therefore the cost.\n",
      "\n",
      "**Bottlenecks & Solutions:**\n",
      "1. **Kafka Partitions**: In Kafka, make sure to use a correct number of partitions in a topic to allow for greater parallelism and throughput.\n",
      "\n",
      "2. **Batch Sizes**: Tuning batch sizes in transformation jobs in Spark Streaming or Apache Beam can help improve performance.\n",
      "\n",
      "3. **Multithreading**: Using multithreading while reading/writing to databases can significantly improve performance.\n",
      "\n",
      "4. **S3 Request Rate**: For a large number of read/write requests, prefixing in S3 bucket can be performed to prevent bottleneck on request rate.  \n",
      "\n",
      "5. **Indexing in Databases**: Choosing the correct indexes in databases can help in efficient querying in the data warehouse, reducing the time for analytics.\n",
      "\n",
      "### Iteration 3 ###\n",
      "\n",
      "Let's breakdown potential improvements in four areas mentioned.\n",
      "\n",
      "**Scalability:**\n",
      "1. **Use Cloud-native Messaging Services**: Although Apache Kafka is a great choice for data streaming services, it might be more scalable and easier to manage if you use cloud-native messaging services like Amazon Kinesis or Google Cloud Pub/Sub. They are highly scalable and fully managed services which eliminate the need to manage the infrastructure.\n",
      "\n",
      "2. **Serverless Architecture**: Adding to automated scaling, using serverless architectures (like AWS Lambda/ Google Cloud Functions) wherever possible can help in efficient resource usage and handling extreme scales seamlessly.\n",
      "\n",
      "**Fault Tolerance:**\n",
      "1. **Multi-region deployment**: Add to the data replication, consider deploying your data pipeline across multiple regions. This ensures that a failure in one region doesn't affect the entire system and provides an additional layer of reliability.\n",
      "  \n",
      "**Cost Efficiency:**\n",
      "1. **Utilize Spot Instances**: In addition to reserved instances, reduced-rate spot instances can further cut costs for non-critical, fault-tolerant aspects of your pipeline that aren't time sensitive.\n",
      "  \n",
      "2. **Data Lifecycle Policies**: Implement data lifecycle policies for your stored data to reduce storage costs over time. Older, less frequently accessed data can be moved to cheaper storage classes.\n",
      "\n",
      "**Bottlenecks & Solutions:**\n",
      "1. **Optimize Underlying Storage Usage**: Depending on your cloud provider and your I/O demands, optimizing the underlying storage technology used by Kafka, and your processing technologies can help relieve bottlenecks.\n",
      "\n",
      "2. **Increase Kafka Producers and Consumers**: You can add more producers and consumers as per the data load. More consumers will be able to handle higher load in parallel.\n",
      "\n",
      "3. **Monitoring and Alerts**: Implement comprehensive monitoring and alerting for your data pipeline. It helps to detect any potential bottlenecks or performance issues proactively before they become critical. You can use services like Amazon CloudWatch or Google Stackdriver for this. You can also consider using specialized tools for monitoring Kafka, like LinkedIn's Cruise Control. \n",
      "\n",
      "4. **Caching**: If your data pipeline involves serving data to end users, consider using a caching layer like Redis or Memcached to reduce load on your databases and make the data serving process more efficient.\n",
      "\n",
      "### Final Optimized Pipeline ###\n",
      "\n",
      "Let's breakdown potential improvements in four areas mentioned.\n",
      "\n",
      "**Scalability:**\n",
      "1. **Use Cloud-native Messaging Services**: Although Apache Kafka is a great choice for data streaming services, it might be more scalable and easier to manage if you use cloud-native messaging services like Amazon Kinesis or Google Cloud Pub/Sub. They are highly scalable and fully managed services which eliminate the need to manage the infrastructure.\n",
      "\n",
      "2. **Serverless Architecture**: Adding to automated scaling, using serverless architectures (like AWS Lambda/ Google Cloud Functions) wherever possible can help in efficient resource usage and handling extreme scales seamlessly.\n",
      "\n",
      "**Fault Tolerance:**\n",
      "1. **Multi-region deployment**: Add to the data replication, consider deploying your data pipeline across multiple regions. This ensures that a failure in one region doesn't affect the entire system and provides an additional layer of reliability.\n",
      "  \n",
      "**Cost Efficiency:**\n",
      "1. **Utilize Spot Instances**: In addition to reserved instances, reduced-rate spot instances can further cut costs for non-critical, fault-tolerant aspects of your pipeline that aren't time sensitive.\n",
      "  \n",
      "2. **Data Lifecycle Policies**: Implement data lifecycle policies for your stored data to reduce storage costs over time. Older, less frequently accessed data can be moved to cheaper storage classes.\n",
      "\n",
      "**Bottlenecks & Solutions:**\n",
      "1. **Optimize Underlying Storage Usage**: Depending on your cloud provider and your I/O demands, optimizing the underlying storage technology used by Kafka, and your processing technologies can help relieve bottlenecks.\n",
      "\n",
      "2. **Increase Kafka Producers and Consumers**: You can add more producers and consumers as per the data load. More consumers will be able to handle higher load in parallel.\n",
      "\n",
      "3. **Monitoring and Alerts**: Implement comprehensive monitoring and alerting for your data pipeline. It helps to detect any potential bottlenecks or performance issues proactively before they become critical. You can use services like Amazon CloudWatch or Google Stackdriver for this. You can also consider using specialized tools for monitoring Kafka, like LinkedIn's Cruise Control. \n",
      "\n",
      "4. **Caching**: If your data pipeline involves serving data to end users, consider using a caching layer like Redis or Memcached to reduce load on your databases and make the data serving process more efficient.\n"
     ]
    }
   ],
   "source": [
    "def refine_pipeline(prompt, iterations=3):\n",
    "    \"\"\"\n",
    "    Iteratively refines a complex data pipeline design based on user constraints.\n",
    "    \n",
    "    :param prompt: Initial prompt to define the problem.\n",
    "    :param iterations: Number of refinement cycles.\n",
    "    :return: Final refined response.\n",
    "    \"\"\"\n",
    "    refined_prompt = prompt\n",
    "    for i in range(iterations):\n",
    "        print(f\"\\n### Iteration {i + 1} ###\\n\")\n",
    "        \n",
    "        # Generate the response\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert cloud data engineer specializing in pipeline optimization.\"},\n",
    "                {\"role\": \"user\", \"content\": refined_prompt},\n",
    "            ],\n",
    "            stream=False  # Non-streaming mode\n",
    "        )\n",
    "        \n",
    "        # Extract the response content\n",
    "        output = response.choices[0].message.content\n",
    "        print(output)\n",
    "        \n",
    "        # Refine the prompt for the next iteration\n",
    "        refined_prompt = f\"\"\"\n",
    "        The previous response provided the following pipeline design:\n",
    "        {output}\n",
    "        \n",
    "        Based on this design, suggest improvements for better scalability, fault tolerance, \n",
    "        and cost efficiency. Also, consider potential bottlenecks and solutions.\n",
    "        \"\"\"\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Initial problem description\n",
    "initial_prompt = \"\"\"\n",
    "I need to design a cloud-based data pipeline for ETL operations:\n",
    "- Data is ingested from multiple sources (APIs, databases, CSVs) into a central storage.\n",
    "- Data transformations include cleaning, aggregation, and enrichment.\n",
    "- Transformed data is stored in a data warehouse (e.g., Snowflake) for analytics.\n",
    "- Constraints: \n",
    "    1. Latency must be under 5 seconds for real-time data.\n",
    "    2. Scalability to handle 10x data growth.\n",
    "    3. Minimize costs while ensuring fault tolerance.\n",
    "Provide an initial pipeline design with technologies and architecture.\n",
    "\"\"\"\n",
    "\n",
    "# Execute the refinement process\n",
    "final_pipeline = refine_pipeline(initial_prompt, iterations=3)\n",
    "\n",
    "print(\"\\n### Final Optimized Pipeline ###\\n\")\n",
    "print(final_pipeline)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f8aaa1-16dd-4a25-aadc-c80da40113a1",
   "metadata": {},
   "source": [
    "## 7. Contrastive Prompting\n",
    "### Concept: Ask the model to compare or contrast multiple inputs.\n",
    "### Use Case: Evaluative tasks, decision-making assistance, or content differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf9d2fb3-f853-4f40-9e3d-48e83c75243a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Contrastive Analysis of Database Solutions ###\n",
      "\n",
      "1. Comparison Table :\n",
      "\n",
      "| Criteria           | Amazon DynamoDB                                                            | PostgreSQL                                         |\n",
      "|------------------- |---------------------------------------------------------------------------|-------------------------------------------------- |\n",
      "| Scalability        | Highly scalable, can handle large-scale, high-throughput workloads.       | Scale-up model may face issues with scalability.  |\n",
      "| Cost               | Pricing based on read/write capacity, storage, data transfer.              | Licensing is free but costs include hardware, maintenance, and personnel. |\n",
      "| Ease of Use        | Simple to set up, automated tasks like backups, scaling and patching.     | May have steep learning curve, requires regular maintenance.  |\n",
      "| High-Throughput Workloads  | Designed for high throughput with microsecond latency. | Can offer good performance but may need extra tuning for high read/write demands.|\n",
      "\n",
      "2. Detailed recommendation:\n",
      "\n",
      "- **Amazon DynamoDB**: This is suitable for applications that need to support high-scale, high-velocity data. If your workload has large-scale, high-throughput requirements with a need for consistent, single-digit millisecond latency at any scale like real-time bidding, gaming, mobile applications, or IoT, DynamoDB would be a good choice. DynamoDB is also suitable for serverless applications or microservices architectures because of its tight integration with other AWS services. Cost management is also easier as it doesn’t require upfront costs and you pay only for the capacity you use.\n",
      "\n",
      "- **PostgreSQL**: It's a better choice for applications that need complex queries, transactions, and relations that are typical in SQL database systems. If you have a workload that requires heavy data analysis or data warehousing with structured relational data, and complex SQL queries, then PostgreSQL would be a better choice. It's a preferable option for on-premise solutions owing to its open-source nature, allowing more control over physical hardware and infrastructure. However, you would have to factor in the cost of managing the servers and other operational aspects which can go high based on the size and complexity of the workload.\n",
      "\n",
      "To summarize, it's not a one-size-fits-all scenario. The choice between DynamoDB and PostgreSQL largely depends on the workload requirements, the nature of the data, cost consideration, and how much control you want to have over your infrastructure.\n"
     ]
    }
   ],
   "source": [
    "# Define the contrastive prompt\n",
    "contrastive_prompt = \"\"\"\n",
    "You are a senior database architect tasked with comparing two data storage solutions: Amazon DynamoDB and PostgreSQL.\n",
    "Evaluate them based on the following criteria:\n",
    "1. Scalability: Ability to handle large-scale, high-throughput workloads.\n",
    "2. Cost: Total cost of ownership, including operational and scaling costs.\n",
    "3. Ease of Use: Learning curve, setup, and maintenance.\n",
    "4. Suitability for High-Throughput Workloads: Performance under high read/write demands.\n",
    "\n",
    "Provide:\n",
    "1. A comparison table summarizing the pros and cons of each solution.\n",
    "2. A detailed recommendation for which option to use in different scenarios.\n",
    "\"\"\"\n",
    "\n",
    "def compare_solutions():\n",
    "    # Generate the response using OpenAI's GPT\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert database architect specializing in cloud and on-premises database solutions.\"},\n",
    "            {\"role\": \"user\", \"content\": contrastive_prompt},\n",
    "        ],\n",
    "        stream=False  # Non-streaming mode\n",
    "    )\n",
    "\n",
    "    # Extract and print the analysis\n",
    "    analysis = response.choices[0].message.content\n",
    "    print(\"\\n### Contrastive Analysis of Database Solutions ###\\n\")\n",
    "    print(analysis)\n",
    "\n",
    "# Run the function\n",
    "compare_solutions()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2a79ca-dc11-4602-9a48-7d83c72593bc",
   "metadata": {},
   "source": [
    "## 8. Instruction-Informed Prompting\n",
    "### Concept: Explicitly instruct the model on what not to do.\n",
    "### Use Case: Controlling style, tone, or output constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85d7ffeb-c094-4c8a-8601-6ba9f2aa6213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Secure API Design ###\n",
      "\n",
      "I recommend designing the API in line with the following specifications:\n",
      "\n",
      "**API Endpoints**\n",
      "\n",
      "1. POST /register\n",
      "2. POST /login\n",
      "3. GET /user\n",
      "4. POST /token/refresh\n",
      "\n",
      "**1. User Registration - POST /register**\n",
      "   \n",
      "The API accepts [username, password] as JSON in POST request. The username should be unique for every user. Passwords will be hashed and salted using bcrypt (or comparable cryptographic algorithm) before being stored.\n",
      "\n",
      "**Response**: Status code 201 if successful, along with a JWT token; 400 if the user already exists.\n",
      "\n",
      "**2. User Login - POST /login**\n",
      "\n",
      "The API accepts [username, password] in a POST request. The API verifies the username and password. The password provided will be hashed and must match the hashed password stored in the system.\n",
      "\n",
      "**Response**: Status code 200 if successful, along with a JWT token; 401 if the username and password are invalid; 404 if the user is not found.\n",
      "\n",
      "**3. User Details - GET /user**\n",
      "\n",
      "This endpoint will decrypt/verify the JWT to ensure it is valid and hasn’t expired. The user information will be retrieved from the database.\n",
      "\n",
      "**Response**: Status code 200 if successful, along with user details; 401 if the token is expired/invalid.\n",
      "\n",
      "**4. Token Refresh - POST /token/refresh**\n",
      "\n",
      "This endpoint is to generate a new access token using the refresh token provided in the request.\n",
      "\n",
      "**Response**: Status 200 if successful, along with a new JWT; 401 if the refresh token is invalid.\n",
      "\n",
      "**Security Mechanisms & OWASP Best Practices:**\n",
      "\n",
      "1. **Principle of Least Privilege**: Each endpoint only has permission to perform the specific actions it needs to deliver its functionality.\n",
      "\n",
      "2. **Input Validation**: All inputs are validated and sanitized to prevent injection attacks.\n",
      "\n",
      "3. **Token Authentication**: JWT tokens are used for authentication that contain a cryptographic signature ensuring the integrity of the information contained in the token.\n",
      "\n",
      "4. **Secure Password Storage**: Passwords are not stored in plain text. They are hashed before storing.\n",
      "\n",
      "5. **Secure Transmission**: All communication will be conducted over HTTPS, ensuring the integrity and confidentiality of the data being sent.\n",
      "\n",
      "6. **Encryption**: Use robust and trusted cryptographic algorithms for password hashing (like bcrypt).\n",
      "\n",
      "7. **Error Handling**: Don’t reveal sensitive information in error messages.\n",
      "\n",
      "8. **Session Management**: JWTs provide a robust way of handling user sessions. To prevent token theft, the refresh token should be stored securely on the server, and new access tokens are generated periodically.\n",
      "\n",
      "This design adheres to the OWASP guidelines for API security and commonly accepted best practices for secure API design. It provides secure user authentication and data protection.\n"
     ]
    }
   ],
   "source": [
    "# Define the instruction-informed prompt\n",
    "instruction_informed_prompt = \"\"\"\n",
    "You are an experienced software engineer specializing in secure API development.\n",
    "Your task is to design an authentication API for a web application. The API should:\n",
    "1. Support user registration and login.\n",
    "2. Implement JWT-based authentication.\n",
    "3. Follow secure design principles.\n",
    "\n",
    "Explicit Instructions:\n",
    "- Do NOT suggest storing passwords in plaintext or reversible encryption.\n",
    "- Do NOT recommend outdated cryptographic algorithms (e.g., MD5, SHA-1).\n",
    "- Ensure compliance with OWASP best practices.\n",
    "\n",
    "Provide:\n",
    "1. A step-by-step API design, including endpoints, request/response formats, and security mechanisms.\n",
    "2. An explanation of how your design adheres to security best practices.\n",
    "\"\"\"\n",
    "\n",
    "def generate_secure_api_design():\n",
    "    # Generate the response using OpenAI's GPT\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a security-focused software engineer tasked with designing secure APIs.\"},\n",
    "            {\"role\": \"user\", \"content\": instruction_informed_prompt},\n",
    "        ],\n",
    "        stream=False  # Non-streaming mode\n",
    "    )\n",
    "\n",
    "    # Extract and print the design\n",
    "    api_design = response.choices[0].message.content\n",
    "    print(\"\\n### Secure API Design ###\\n\")\n",
    "    print(api_design)\n",
    "\n",
    "# Run the function\n",
    "generate_secure_api_design()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a4ff70-8524-4cbd-8d2f-64fad2176dd7",
   "metadata": {},
   "source": [
    "## 9. Recursive Question Answering\n",
    "### Concept: Use the model's output to generate additional queries to refine understanding.\n",
    "### Use Case: Exploring topics in-depth, educational contexts, or reasoning augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2920ec62-cd5e-46a5-a5b5-cc7592c7c859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Iteration 1 ###\n",
      "\n",
      "Several factors might be causing the slow performance during peak traffic, including inefficient database schema, long-running transactions, misconfigured parameters, or inefficient queries. \n",
      "\n",
      "Here's a general troubleshooting guide:\n",
      "\n",
      "1. **Monitor Database Performance**: Use database monitoring tools like `pg_stat_statements` to examine the overall health of your database. This can provide information about slow-running queries, row read/writes, etc.\n",
      "\n",
      "2. **Identify slow SQL queries**: Slow queries are a common performance bottleneck. Use the PostgreSQL’s `EXPLAIN` and `EXPLAIN ANALYSE` commands to understand how queries are executed and where you can optimize.\n",
      "\n",
      "3. **Check for long-running transactions**: Long-running transactions can block other operations and consume system resources. Identify long-running transactions using PostgreSQL's `pg_stat_activity` view.\n",
      "\n",
      "4. **Review your Indexes**: Having the right indexes for your data is critical. Use `pg_stat_user_indexes` to check index usage statistics. This can tell you if indexes are being under-utilized, or if additional indexes could help.\n",
      "\n",
      "5. **Locks and Deadlocks**: Check for frequent locking conflicts between transactions. PostgreSQL’s `pg_locks` view can be used to identify locks in your system.\n",
      "\n",
      "6. **Analyze your hardware usage**: Check the system's CPU, Disk I/O, and memory usage. If any of these resources are constantly high, you might need an upgrade.\n",
      "\n",
      "7. **Database Parameters**: Review and tune PostgreSQL parameters such as shared_buffers, work_mem, effective_cache_size, and checkpoint-related parameters.\n",
      "\n",
      "8. **Table Bloat**: Large updates or deletes can lead to table bloat. Use PostgreSQL's built-in tools like `VACUUM` and `VACUUM FULL` to remove that bloat.\n",
      "\n",
      "9. **Connection Management**: Check whether the maximum number of connections are frequently being used. Consider using a connection pooling mechanism to maximize the usage of connections.\n",
      "\n",
      "10. **Partition Large Tables**: Consider partitioning very large tables which can improve query performance. \n",
      "\n",
      "Always ensure you test any changes made in a controlled environment before implementing them in production to avoid unintended side-effects or exacerbating the performance problem.\n",
      "\n",
      "### Iteration 2 ###\n",
      "\n",
      "Based on the general guide, here are the specific steps that can be taken to pinpoint the issues:\n",
      "\n",
      "1. **Monitor database performance regularly**: Use dedicated tools to monitor the performance regularly during peak and low traffic to spot any trends or abnormalities. It may be a good idea to check the slow query logs daily during peak traffic periods. \n",
      "\n",
      "2. **Identify slow SQL queries and optimize them**: Use the `EXPLAIN` and `EXPLAIN ANALYSE` commands to find and optimize slow queries. You may consider rewriting the queries or introducing indexes to speed up execution.\n",
      "\n",
      "3. **Audit your transaction duration**: Use `pg_stat_activity` view frequently to identify any long-lasting transactions. Consider tuning the duration of your transactions and optimize them to prevent them from hogging system resources.\n",
      "\n",
      "4. **Check if you have proper indexing in place**: Audit your current indexes through `pg_stat_user_indexes` and implement or update indexes based on the results. \n",
      "\n",
      "5. **Investigate lock and deadlock issues**: Regularly check `pg_locks` for any locks that might be slowing down your system. Resolve these issues as they happen. \n",
      "\n",
      "6. **Keep an eye on your resource usage**: If the system resources are constantly high even during low traffic, then it’s a clear indication the system needs to be upgraded to handle larger loads. \n",
      "\n",
      "7. **Review your database parameters**: If they have not been reviewed or updated in a while, they may not be optimized for your current workload. You should gradually adjust parameters such as shared_buffers and work_mem and monitor the impact on your performance.\n",
      "\n",
      "8. **Handle the table bloat**: Implement regular ‘vacuuming’ of your tables by using `VACUUM` or `VACUUM FULL` commands.\n",
      "\n",
      "9. **Manage your connections efficiently**: Implement connection pooling mechanism like PgBouncer to efficiently manage increasing loads during peak times.\n",
      "\n",
      "10. **Partition large tables**: If your database has large tables, consider partitioning them into smaller, more manageable ones.\n",
      "\n",
      "In conclusion, the most probable reasons for the slow performance during peak traffic are the inefficient SQL-queries, frequent long-lasting transactions, and over-utilization of system resources. Target these areas first, monitor the improvements, and then move on to the next issues if needed.\n",
      "\n",
      "### Iteration 3 ###\n",
      "\n",
      "Based on the given information, key areas that need to be addressed and my targeted actions for performance improvement are:\n",
      "\n",
      "1. **Slow SQL Queries:** Besides frequently checking for slow queries and optimizing them, consider setting up alerting for any queries that cross a certain threshold. This will help in proactively identifying and addressing the issues. Also, look into frequently run queries. Just because a query isn’t slow doesn’t mean it doesn’t have an impact. If it is run very frequently, it may be beneficial to optimize it as well.\n",
      "\n",
      "2. **Transactions Duration:** Long-lasting transactions can lead to a lot of system performance issues. A transaction lock can cause a ripple effect on other transactions waiting for resources to be released. You should identify such transactions and optimize them. Also, setting up alerts for long-lasting transactions can help in better resource management during peak times.\n",
      "\n",
      "3. **Inefficient Indexing:** Continuous auditing of the current indexes is recommended. Additionally, automating the process to identify and rebuild inefficient indexes can save a lot of manual hours and also improve overall system performance.\n",
      "\n",
      "4. **System Resources Over-utilization:** If the database system is constantly over-utilizing the resources during peak traffic, consider adding more resources or set up an auto-scaling system. \n",
      "\n",
      "5. **Table Partitioning:** Large tables can slow down your queries, affect your indexes, and ultimately your application performance. Partition large tables into more manageable ones. Consider adding this as part of the regular database maintenance routine. \n",
      "\n",
      "6. **Connection Pooling:** If you don't already have it, implement a connection pooling mechanism. If the current one is not sufficient consider optimizing the settings or use a more efficient mechanism.\n",
      "\n",
      "Remember to involve the system's relevant stakeholders such as the application developers and system admins during this optimization process because this is a lot more than just a one-man job.\n",
      "\n",
      "### Final Diagnosis and Recommendations ###\n",
      "\n",
      "Based on the given information, key areas that need to be addressed and my targeted actions for performance improvement are:\n",
      "\n",
      "1. **Slow SQL Queries:** Besides frequently checking for slow queries and optimizing them, consider setting up alerting for any queries that cross a certain threshold. This will help in proactively identifying and addressing the issues. Also, look into frequently run queries. Just because a query isn’t slow doesn’t mean it doesn’t have an impact. If it is run very frequently, it may be beneficial to optimize it as well.\n",
      "\n",
      "2. **Transactions Duration:** Long-lasting transactions can lead to a lot of system performance issues. A transaction lock can cause a ripple effect on other transactions waiting for resources to be released. You should identify such transactions and optimize them. Also, setting up alerts for long-lasting transactions can help in better resource management during peak times.\n",
      "\n",
      "3. **Inefficient Indexing:** Continuous auditing of the current indexes is recommended. Additionally, automating the process to identify and rebuild inefficient indexes can save a lot of manual hours and also improve overall system performance.\n",
      "\n",
      "4. **System Resources Over-utilization:** If the database system is constantly over-utilizing the resources during peak traffic, consider adding more resources or set up an auto-scaling system. \n",
      "\n",
      "5. **Table Partitioning:** Large tables can slow down your queries, affect your indexes, and ultimately your application performance. Partition large tables into more manageable ones. Consider adding this as part of the regular database maintenance routine. \n",
      "\n",
      "6. **Connection Pooling:** If you don't already have it, implement a connection pooling mechanism. If the current one is not sufficient consider optimizing the settings or use a more efficient mechanism.\n",
      "\n",
      "Remember to involve the system's relevant stakeholders such as the application developers and system admins during this optimization process because this is a lot more than just a one-man job.\n"
     ]
    }
   ],
   "source": [
    "def recursive_qa(initial_prompt, iterations=3):\n",
    "    \"\"\"\n",
    "    Uses recursive question answering to iteratively refine understanding of a complex issue.\n",
    "\n",
    "    :param initial_prompt: The initial question or problem statement.\n",
    "    :param iterations: Number of refinement cycles.\n",
    "    :return: Final refined understanding.\n",
    "    \"\"\"\n",
    "    current_prompt = initial_prompt\n",
    "    all_responses = []\n",
    "\n",
    "    for i in range(iterations):\n",
    "        print(f\"\\n### Iteration {i + 1} ###\\n\")\n",
    "        \n",
    "        # Generate a response from the model\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert database engineer specializing in performance optimization.\"},\n",
    "                {\"role\": \"user\", \"content\": current_prompt},\n",
    "            ],\n",
    "            stream=False\n",
    "        )\n",
    "        \n",
    "        # Extract and print the output\n",
    "        output = response.choices[0].message.content\n",
    "        print(output)\n",
    "        all_responses.append(output)\n",
    "        \n",
    "        # Generate a follow-up prompt based on the model's output\n",
    "        current_prompt = f\"\"\"\n",
    "        Based on the response:\n",
    "        {output}\n",
    "        \n",
    "        Refine the analysis. Identify specific root causes or areas for further investigation. \n",
    "        Suggest targeted actions for performance improvement.\n",
    "        \"\"\"\n",
    "    \n",
    "    return all_responses\n",
    "\n",
    "# Define the initial problem statement\n",
    "initial_prompt = \"\"\"\n",
    "Our PostgreSQL database is experiencing slow query performance during peak traffic. \n",
    "Key characteristics:\n",
    "- High number of concurrent reads and writes.\n",
    "- Tables contain millions of rows.\n",
    "- Indexes exist but may not be optimized.\n",
    "\n",
    "What could be causing the issue? Provide general troubleshooting steps.\n",
    "\"\"\"\n",
    "\n",
    "# Execute the recursive QA process\n",
    "final_responses = recursive_qa(initial_prompt, iterations=3)\n",
    "\n",
    "print(\"\\n### Final Diagnosis and Recommendations ###\\n\")\n",
    "print(final_responses[-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984f885c-b582-4a3d-850d-8857ec9b2167",
   "metadata": {},
   "source": [
    "## 10. Prompt Optimization via Anchoring (Selective Prompt Anchoring - SPA)\n",
    "### Concept: Anchor the model's response around specific terms, phrases, or ideas to guide focus.\n",
    "### Use Case: Forcing focus on key attributes in technical or evaluative discussions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "932c6941-9c74-487a-a570-f394993ba2e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Anchored Response on Scalability ###\n",
      "\n",
      "1. Proposed Architecture:\n",
      "The system will use a microservice architecture where different functional modules of the system will be designed as isolated services capable of being scaled independently.\n",
      "\n",
      "Database: To handle high read and write throughput efficiently, we can adopt a “NoSQL” database like Apache Cassandra or MongoDB, which can be distributed over a cluster of servers and allows linear scalability in terms of throughput. These databases follow an eventually consistent model allowing us to make trade-offs between consistency, availability and partition tolerance based on CAP theorem.  \n",
      "\n",
      "Caching: We can make use of a distributed caching system such as Redis or Memcached to cache frequently accessed data, thereby reducing read latency and database load.\n",
      "\n",
      "Message Queuing: To handle write intensive workloads, we can use message queuing systems like Apache Kafka or RabbitMQ which can serve as a buffer for data and ensure durability. \n",
      "\n",
      "Load Balancer: To distribute the traffic to different instances, a load balancer will be used. Something like Nginx or HAProxy can efficiently distribute incoming network traffic across a group of backend servers or microservices to ensure no single server carries too much load.\n",
      "\n",
      "Auto-scaler: To ensure horizontal scaling, we would include an auto-scaler, which monitors the load on the system and automatically adds or removes instances based on pre-defined rules.\n",
      "\n",
      "2. Justification for the Architectural Choices:\n",
      "\n",
      "Microservices allow us to horizontally scale up or down individual components of the system based on demand, rather than scaling the entire system as a whole. Every instance of each service is identical and stateless; this allows the system to distribute the load evenly across all instances.\n",
      "\n",
      "NoSQL databases like Apache Cassandra provide high availability and partition tolerance, but trade off consistency - a compromise made according to the CAP theorem. They also allow sharding/partitioning of data over the nodes, hence, when we add a new machine - we achieve scalability.\n",
      "\n",
      "Using a distributed caching system like Redis or Memcached improves lookup performance and relieves pressure from read-intensive operations on the database.\n",
      "\n",
      "Message queuing systems can hold a large number of write requests and process them asynchronously, thereby enhancing the system's write throughput.\n",
      "\n",
      "The load balancer ensures that incoming network traffic is efficiently distributed across multiple servers or microservices, thereby preventing any single application instance from becoming a bottleneck.\n",
      "\n",
      "Auto-scalers add or remove instances depending upon the current load, which aids in providing horizontal scaling to the architecture - thereby making sure the system performs reliably even with increases in load. \n",
      "\n",
      "Overall, each component of the architecture is chosen to emphasize scalability. The real-world usage of the provided systems in scalable architectures further reiterates their suitability.\n"
     ]
    }
   ],
   "source": [
    "def anchored_prompt():\n",
    "    # Define the anchored prompt\n",
    "    prompt = \"\"\"\n",
    "You are a software architect specializing in distributed systems.\n",
    "The goal is to design a system that prioritizes scalability above all other factors.\n",
    "\n",
    "Anchoring Terms: \n",
    "- Horizontal scaling\n",
    "- Partitioning\n",
    "- Load balancing\n",
    "- Consistency trade-offs (CAP theorem)\n",
    "\n",
    "Task:\n",
    "1. Propose an architecture for a distributed system handling high read and write throughput.\n",
    "2. Justify all architectural choices using the anchoring terms.\n",
    "3. Avoid focusing on non-scalability-related concerns such as cost or simplicity.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate the response from OpenAI\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a software architect focused on designing scalable systems.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        stream=False\n",
    "    )\n",
    "\n",
    "    # Extract and display the output\n",
    "    output = response.choices[0].message.content\n",
    "    print(\"\\n### Anchored Response on Scalability ###\\n\")\n",
    "    print(output)\n",
    "\n",
    "# Execute the function\n",
    "anchored_prompt()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b665c7eb-8d33-41b5-ae37-8acd53652aeb",
   "metadata": {},
   "source": [
    "## 11. Knowledge Injection\n",
    "### Concept: Include relevant, external knowledge within the prompt to expand the model's scope.\n",
    "### Use Case: Domain-specific insights or ensuring the model works with updated context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da9cce25-77ff-4e45-b8dc-d84083003211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Optimized Kafka Pipeline Design ###\n",
      "\n",
      "Here is a proposed design for real-time processing of 1 million messages per second using Apache Kafka.\n",
      "\n",
      "Design:\n",
      "1. Kafka Producers: Use Apache Kafka's high-throughput Producers API to ingest data into the pipeline at an astounding rate. Make use of the `linger.ms` property to take advantage of batching to reduce network overhead. Set it to an optimal value such as a few milliseconds to combine more messages into one request and reduce the number of requests. \n",
      "\n",
      "2. Kafka Brokers: Increase the number of brokers to distribute the load, as Kafka can scale horizontally with the addition of more brokers. Each broker can hold a number of partitions for each topic.\n",
      "\n",
      "3. Topic Partitioning: Create multiple partitions per topic to permit parallel processing of data. For instance, if there are P partitions, we can have at most P consumers within a Consumer Group actively consuming data.\n",
      "\n",
      "4. Replication: Ensure each Kafka topic is replicated across different brokers to tolerate failure of a broker. This would hence ensure durability and high availability of data.\n",
      "\n",
      "5. Compaction: Enable Log Compaction for topics we intend to have act as a long-term, fault-tolerant storage. This feature will keep the latest version of a record, thereby minimizing the storage requirements.\n",
      "\n",
      "6. Consumer Groups: Consumers belong to a consumer group. Each message in Kafka can be consumed by one consumer within each subscribing consumer group.\n",
      "\n",
      "7. Monitoring and Management: Utilize Kafka’s built-in tools, like Kafka Streams API, to handle real-time data feed, streaming, and processing.\n",
      "\n",
      "Alignment with Kafka Best Practices:\n",
      "1. High Throughput & Scalability: The use of multiple partitions and brokers, along with batching at the producer level, allows the pipeline design to handle a high rate of incoming data.\n",
      "2. Durability and Fault Tolerance: Replication ensures that data is safe and available even in the case of broker failure.\n",
      "3. Low Latency: Efficient handling of real-time data feed is achieved through the Kafka Streams API.\n",
      "4. Storage Efficiency: By enabling log compaction, we achieve efficient storage use by reducing the storage requirements for large topics.\n"
     ]
    }
   ],
   "source": [
    "def knowledge_injection():\n",
    "    # External knowledge to inject\n",
    "    kafka_knowledge = \"\"\"\n",
    "Apache Kafka is a distributed streaming platform that enables:\n",
    "1. **High Throughput**: It supports massive amounts of data due to its distributed architecture.\n",
    "2. **Low Latency**: It efficiently handles real-time data streaming.\n",
    "3. **Durability**: Messages are written to disk and replicated across multiple brokers.\n",
    "4. **Scalability**: Kafka can scale horizontally by adding more brokers.\n",
    "\n",
    "Key Features for Optimization:\n",
    "- **Partitions and Replication**: Partitioning ensures parallel processing, while replication provides fault tolerance.\n",
    "- **Consumer Groups**: Multiple consumers in a group can process messages in parallel for scalability.\n",
    "- **Compaction**: Configuring log compaction reduces storage requirements for large topics.\n",
    "- **Batching**: Use batching to minimize network overhead during message production.\n",
    "\"\"\"\n",
    "\n",
    "    # Define the injected prompt\n",
    "    prompt = f\"\"\"\n",
    "You are a data engineering expert. Your task is to design an optimized data pipeline using Apache Kafka.\n",
    "\n",
    "Background Knowledge:\n",
    "{kafka_knowledge}\n",
    "\n",
    "Task:\n",
    "1. Design a Kafka-based data pipeline for processing 1 million messages per second in real-time.\n",
    "2. Ensure fault tolerance and high availability.\n",
    "3. Optimize for storage efficiency and low latency.\n",
    "4. Explain how the proposed design aligns with Kafka's best practices.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate the response\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a data engineering expert.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        stream=False\n",
    "    )\n",
    "\n",
    "    # Extract and display the output\n",
    "    output = response.choices[0].message.content\n",
    "    print(\"\\n### Optimized Kafka Pipeline Design ###\\n\")\n",
    "    print(output)\n",
    "\n",
    "# Execute the function\n",
    "knowledge_injection()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9bf53d-5ce5-48ad-bed1-87849821a5f2",
   "metadata": {},
   "source": [
    "## 12. Meta-Prompting\n",
    "### Concept: Instruct the AI to self-reflect on its output and improve it.\n",
    "### Use Case: Code reviews, debugging, or enhancing creative content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d3fb04b-fefa-4963-aa30-45017a859683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Initial Response ###\n",
      "\n",
      "**System Architecture Diagram (Described in Text)**\n",
      "\n",
      "1. **Load Balancer Layer**: A group of Load balancers that accepts incoming traffic and evenly distributes it across multiple service instances.\n",
      "\n",
      "2. **Service Layer**: Multiple microservice instances that are triggered by the load balancer. These are stateless services that each handle a piece of functionality (e.g., user authentication, product catalog, order management, etc.)\n",
      "\n",
      "3. **Data Persistence Layer**: This includes databases and storage systems like SQL or NoSQL databases and file storage systems (for images, videos, etc.)\n",
      "\n",
      "4. **Message Queuing Layer**: A system that helps inter-service communication.\n",
      "\n",
      "5. **Caching Layer**: Systems like Redis or Memcache to speed requests by caching common requests and their results.\n",
      "\n",
      "**Key Technologies and Their Roles**:\n",
      "1. **AWS / Google Cloud / Azure**: These cloud platforms provide robust, scalable, and distributed computing capabilities. We can leverage their compute instances, storage, and database solutions and their microservices-based tools like AWS Lambda, Google Cloud Functions or Azure Functions.\n",
      "\n",
      "2. **Docker/Kubernetes**: Docker will be used for creating lightweight, standalone, and secure microservice containers. Kubernetes will be used for orchestrating these containers, help in scaling easily under load, and recovering from failures.\n",
      "\n",
      "3. **Load Balancers (AWS ELB, Google Load balancer)**: Distribute incoming traffic to prevent any single instance from being the bottleneck. \n",
      "\n",
      "4. **RabbitMQ/Kafka**: Message Queuing Systems to handle inter-service communication asynchronously and reliably.\n",
      "\n",
      "5. **Redis/Memcache**: To cache repeated requests and improve the system's overall performance.\n",
      "\n",
      "6. **Consul / Eureka**: For Service Discovery.\n",
      "\n",
      "**Strategies to Achieve Fault Tolerance**:\n",
      "\n",
      "1. **Redundancy**: Deploy multiple instances of each microservice and databases, distributed across various zones and regions.\n",
      "\n",
      "2. **Load Balancing**: Equally distribute incoming traffic among all service instances to prevent any single point of failure.\n",
      "\n",
      "3. **Auto-scaling**: Automatically scale up or down the number of compute instances or services based on the load.\n",
      "\n",
      "4. **Health Checks**: Regularly check the wellness of each service in the system and if any service goes down, it should be replaced immediately.\n",
      "\n",
      "5. **Database Replication**: Database should be replicated and should support multi-zone availability to prevent data loss.\n",
      "\n",
      "6. **Circuit Breaker Pattern**: In microservices, to prevent a network or service failure from cascading to other services, we could use the Circuit Breaker pattern. \n",
      "\n",
      "7. **Data Backup and Recovery**: Regular backup of data in a secure location and have recovery mechanisms in place to fetch that data when required.\n",
      "\n",
      "8. **Message Queues**: Even if a service is down, messages (tasks) should not be lost. They should be persisted in queues and processed when the service is up again. \n",
      "\n",
      "These strategies and technologies will ensure high availability, recovery from failures, and efficient scaling under load.\n",
      "\n",
      "### Self-Reflection ###\n",
      "\n",
      "The original response is detailed, thorough, and demonstrates deep knowledge of the topic. It is well-structured, which makes it easy to understand system operations, as well as different layers and corresponding technologies used in this system architecture. The key strategies and technologies detailed provide the necessary mechanisms for a fault-tolerant system.\n",
      "\n",
      "However, there may be room for improvement in few areas:\n",
      "\n",
      "1. **Clarity**: While the output is clear for someone with technical knowledge, for a non-technical audience, explaining what specific terms mean (like Load Balancer Layer, stateless services, Docker/Kubernetes, etc.) could make the response more accessible.\n",
      "\n",
      "2. **Completeness**: The output is generally comprehensive, but specifics about monitoring and logging processes, security aspects as well as the specification/use of any APIs can be incorporated. Also, any dependencies between components or the workflow of a typical request can be added to complete the overall architecture.\n",
      "\n",
      "3. **Relevance**: The technologies and strategies suggested are modern and well recognized in the industry. However, a discussion on why these specific technologies were chosen over other alternatives could provide more depth to the response.\n",
      "\n",
      "Suggested improvements could include:\n",
      "\n",
      "a. Glossary/Explanation of terms: Define technical terms inline or provide a glossary at the bottom. \n",
      "\n",
      "b. Workflow/Dependency diagram: A complementary visual diagram describing the system flow or dependencies among the services/components could aid understanding.\n",
      "\n",
      "c. Comparison/Justification of technology: Discussing why certain technologies were chosen over others, providing a comparison with potential alternatives.\n",
      "\n",
      "d. More Detail on APIs: If APIs are used, specify what they are used for, how they work, and any relevant standards or best practices applied.\n",
      "\n",
      "e. Security Measures: Address security measures adopted in various aspects of the architecture.\n",
      "\n",
      "f. Monitoring and Logging: Discuss strategies for monitoring system health, as well as logging and analyzing system use and potential issues.\n",
      "\n",
      "Incorporating these suggestions will enhance the response for both technical and non-technical audiences.\n",
      "\n",
      "### Refined Response ###\n",
      "\n",
      "**Refined System Architecture Diagram (Explained in Text)**\n",
      "\n",
      "Let me clarify the system architecture using five distinct layers:\n",
      "\n",
      "1. **Load Balancer Layer**: This layer consists of load balancers, which are tools that manage the traffic coming into the system by distributing it evenly across multiple service instances. This helps prevent any single part of the system from becoming overwhelmed with traffic.\n",
      "\n",
      "2. **Service Layer**: Here we have several stateless services, which are services that don't store user data from one session for use in another. Each service performs a particular function, such as user authentication, managing the product catalog, or taking care of order management.\n",
      "\n",
      "3. **Data Persistence Layer**: This layer includes our databases and storage systems. We use SQL or NoSQL databases for structured data and file storage systems (for images, videos, etc.) for unstructured data.\n",
      "\n",
      "4. **Message Queuing Layer**: This is a system that makes the communication between various services in our system easy and efficient.\n",
      "\n",
      "5. **Caching Layer**: Here we use systems like Redis or Memcache that make our system faster by storing common requests and their results, preventing the need to compute the same results again and again.\n",
      "\n",
      "Each of these technical components is fulfilled by technologies such as:\n",
      "\n",
      "1. **Cloud Platforms (AWS / Google Cloud / Azure)**: These provide us with the necessary computational power and storage we need. They also have specialized tools for working with microservices (small, specific functions performed by separate services) like AWS Lambda, Google Cloud Functions or Azure Functions.\n",
      "\n",
      "2. **Containerization and Orchestration (Docker/Kubernetes)**: Docker allows us to create microservices in separate, secure \"containers\" that can run on any system. Kubernetes helps us manage these containers, making sure that they scale correctly under increased load and recover quickly from failures.\n",
      "\n",
      "3. **Load Balancers (AWS ELB, Google Load balancer)**: These tools help us distribute traffic evenly among various services to maintain a balanced load.\n",
      "\n",
      "4. **Messaging Systems (RabbitMQ/Kafka)**: These help us handle communication between services asynchronously (i.e., each service can operate independently of the others) and reliably.\n",
      "\n",
      "5. **Caching Tools (Redis/Memcache)**: These systems improve performance by storing common requests and their results, saving the time needed to compute these results again.\n",
      "\n",
      "6. **Service Discovery (Consul / Eureka)**: These help us keep track of all the services running in our system and their location (their IP addresses and ports), making communication between services easier.\n",
      "\n",
      "To ensure the system operates smoothly at all times, we employ the following fault tolerance strategies:\n",
      "\n",
      "1. **Redundancy**: Running multiple instances of each service across different geographic locations for backup and to manage service disruptions.\n",
      "\n",
      "2. **Load Balancing**: Using load balancers to fairly distribute incoming traffic among all service instances to avoid overburdening a single instance.\n",
      "\n",
      "3. **Auto-scaling**: The ability to scale the number of compute instances or services based on the load.\n",
      "\n",
      "4. **Health Checks**: Frequent checks of each service's \"health\" or functionality. If a service goes down, it gets replaced immediately.\n",
      "\n",
      "5. **Database Replication**: Creating copies of databases across multiple locations to prevent data loss during system failures.\n",
      "\n",
      "6. **Circuit Breaker Pattern**: A method to prevent a failure in one service from affecting others.\n",
      "\n",
      "7. **Data Backup and Recovery**: Regularly backing up data in a secure location and having mechanisms to restore this data in case of loss.\n",
      "\n",
      "8. **Message Queues**: In case a service goes down, tasks should not be lost. They are saved in a queue to be processed once the service is back up and running.\n",
      "\n",
      "Additional considerations are made to ensure system security, including encryption at rest and in transit, secure design principles, and least privilege access for staff. API usage follows industry best standards. Our monitoring and logging systems provide visibility into system health and potential issues.\n"
     ]
    }
   ],
   "source": [
    "def meta_prompting():\n",
    "    # Step 1: Define the primary task prompt\n",
    "    task_prompt = \"\"\"\n",
    "    You are a senior cloud architect. Design a fault-tolerant microservices architecture for a system \n",
    "    that handles millions of user requests per day. Key requirements:\n",
    "    1. High availability\n",
    "    2. Automatic recovery from failures\n",
    "    3. Efficient scaling under load\n",
    "    \n",
    "    Output should include:\n",
    "    - System architecture diagram (describe in text)\n",
    "    - Key technologies and their roles\n",
    "    - Strategies to achieve fault tolerance\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 2: Define the self-reflection prompt\n",
    "    self_reflection_prompt = \"\"\"\n",
    "    Reflect on the output based on the following criteria:\n",
    "    1. Clarity: Is the output clear and understandable?\n",
    "    2. Completeness: Does it address all key requirements in the task?\n",
    "    3. Relevance: Are the suggested technologies and strategies appropriate?\n",
    "    \n",
    "    Suggest specific improvements to enhance the response.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 3: Prompt for improvements\n",
    "    improvement_prompt_template = \"\"\"\n",
    "    Original response:\n",
    "    {original_response}\n",
    "    \n",
    "    Suggested improvements:\n",
    "    {improvement_suggestions}\n",
    "    \n",
    "    Refine the response based on the suggested improvements.\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate the initial response\n",
    "    initial_response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert in designing cloud architectures.\"},\n",
    "            {\"role\": \"user\", \"content\": task_prompt},\n",
    "        ],\n",
    "        stream=False\n",
    "    ).choices[0].message.content\n",
    "\n",
    "    print(\"\\n### Initial Response ###\\n\")\n",
    "    print(initial_response)\n",
    "\n",
    "    # Self-reflection on the initial response\n",
    "    reflection_response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert in evaluating technical outputs.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"{self_reflection_prompt}\\n\\nOriginal response:\\n{initial_response}\"},\n",
    "        ],\n",
    "        stream=False\n",
    "    ).choices[0].message.content\n",
    "\n",
    "    print(\"\\n### Self-Reflection ###\\n\")\n",
    "    print(reflection_response)\n",
    "\n",
    "    # Generate the refined response based on suggested improvements\n",
    "    refined_prompt = improvement_prompt_template.format(\n",
    "        original_response=initial_response,\n",
    "        improvement_suggestions=reflection_response\n",
    "    )\n",
    "    refined_response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert in refining technical designs.\"},\n",
    "            {\"role\": \"user\", \"content\": refined_prompt},\n",
    "        ],\n",
    "        stream=False\n",
    "    ).choices[0].message.content\n",
    "\n",
    "    print(\"\\n### Refined Response ###\\n\")\n",
    "    print(refined_response)\n",
    "\n",
    "# Execute the meta-prompting function\n",
    "meta_prompting()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a502815d-366d-4ee9-8ce0-ea7f83381598",
   "metadata": {},
   "source": [
    "## 13. Controlled Randomization\n",
    "### Concept: Use randomness creatively by ask\n",
    "### Use Case: Creative tasks requiring diverse yet focused outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4405e207-fa4e-4965-bc27-d73eb81345af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Randomized Sharding Strategies ###\n",
      "\n",
      "Strategy #1: Geodetic Sharding\n",
      "This strategy takes inspiration from the real-world model of the Earth's geodesic grid. We stop viewing our data as rows in tables and start viewing it as points on a globe. Each node in our distributed system will be responsible for a particular slice of this grid. This shard assignment mechanism ensures an even distribution of data because geodesic grids themselves are divided into evenly-sized cells. For adding more capacity, new nodes can take over some regions from the existing nodes akin to plate tectonics. The randomness is introduced by GRE (Geography Related Events), such as of a sudden population boom in a specific region or a disastrous event leading to population decline.\n",
      "\n",
      "ACID properties should be enforced using a distributed transaction protocol like Two-phase commit or Three-phase commit. For fault tolerance, one could use an erasure coding-based data replication for each slice.\n",
      "\n",
      "Strategy #2: Behavioral Sharding\n",
      "In this strategy, we use an individual user's or client's behavior as a metric to shard our data. For example, we could take the 'rate of data creation' for individual users into account and distribute it across our nodes. If certain users are creating massive amounts of data efficiently, we adjust our sharding strategy by assigning more nodes to handle these 'power users.' This mimics the random phenomena of place popularity in the world where certain places spontaneously become popular due to unpredictable reasons and suddenly experience an influx of people. It ensures fault tolerance by reducing the effects of hotspots and also allows easy scale-up by adding more nodes to manage heavy-duty users.\n",
      "\n",
      "Distributed transactions can be handled with optimistic concurrency control + two-phase commit for ACID. Timestamps used to log user behaviour can be used to enforce eventual consistency.\n",
      "\n",
      "Strategy #3: Quantum Sharding\n",
      "This strategy draws inspiration from the peculiar properties of quantum objects. Here, we use a superposition of multiple sharding schemes simultaneously. We can think of a shard id attached to each row as a qubit which is in a superposition of residing in all nodes at the same time. When we need to retrieve or write a row, we \"collapse\" this superposition to a single node, based on a quantum-inspired probabilistic algorithm. This brings randomness into play in an essential and intricate manner.\n",
      "\n",
      "The enforcement of ACID properties would be challenging but can be done by considering the quantum nature of data during the design of a distributed transaction protocol. For instance, a modified version of the two-phase commit could be employed. Furthermore, to handle issues of fault tolerance and scalability, one could leverage quantum error correction principles and concepts of quantum entanglement.\n",
      "\n",
      "The implementation of this strategy would require significant advances in quantum computations and technology, making it a long-term, futuristic consideration.\n"
     ]
    }
   ],
   "source": [
    "def controlled_randomization():\n",
    "    # Define the initial structured prompt\n",
    "    prompt = \"\"\"\n",
    "You are a database architecture expert. Your task is to propose creative sharding strategies \n",
    "for a distributed database designed to handle billions of records. \n",
    "\n",
    "Guidelines:\n",
    "1. The strategies must ensure even distribution of data.\n",
    "2. They should account for fault tolerance and ease of scaling.\n",
    "3. Introduce randomness to explore unconventional approaches.\n",
    "\n",
    "Constraints:\n",
    "- Each strategy must adhere to ACID or eventual consistency.\n",
    "- Use real-world analogies or examples when possible.\n",
    "- Output three unique strategies with different approaches.\n",
    "\n",
    "Introduce a random twist in each suggestion by including non-traditional criteria (e.g., user behavior patterns, geographic quirks).\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate the response\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert in innovative database architectures.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        stream=False\n",
    "    )\n",
    "\n",
    "    # Extract the output\n",
    "    output = response.choices[0].message.content\n",
    "\n",
    "    print(\"\\n### Randomized Sharding Strategies ###\\n\")\n",
    "    print(output)\n",
    "\n",
    "# Execute the function\n",
    "controlled_randomization()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb1257c-eb14-4fcf-aba4-add229fff491",
   "metadata": {},
   "source": [
    "## 14. Layered Contextual Prompts\n",
    "### Concept: Build prompts that work in nested layers, where each layer sets up deeper context.\n",
    "### Use Case: Progressive learning or layered content generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e5caf643-4ec4-43b8-ad80-45e39db21fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Layer 1: High-Level Architecture ###\n",
      "\n",
      "1. High-level Architecture Diagram in Textual Form:\n",
      "----------------------------------------------------------------------------\n",
      "\n",
      "There are three main parts to this architecture: log generation, log collection, and log analysis.\n",
      "\n",
      "1. **Log Generation**: Every microservice in each region will be configured to generate logs for their operations and activities. \n",
      "\n",
      "2. **Log Collection**: A log shipper, such as Fluentd or Logstash, will be deployed alongside each microservice. These are responsible for collecting logs generated by microservices and pushing them to a log aggregation/centralization system.\n",
      "\n",
      "3. **Log Aggregation/Centralization**: Here, we'll utilize a scalable, distributed storage system like Elasticsearch. This is where all the logs from different microservices across all regions will be stored and indexed. \n",
      "\n",
      "4. **Log Analysis**: For analyzing and visualization of the logs, we can use Kibana, which can interact with Elasticsearch easily. \n",
      "\n",
      "----------------------------------------------------------------------------\n",
      "2. Technologies Involved and Their Roles:\n",
      "\n",
      "1. **Microservices**: They are the basic components of our application performing various duties. They are responsible for log generation. Stack traces, Operation data, execution paths, etc., are collected and noted down in the attached log shipper.\n",
      " \n",
      "2. **Fluentd or Logstash**: These are log shippers designed to efficiently collect, parse and structure logs in a consistent manner, regardless of source or format. It helps to unifies the data collection and consumption for a better use and understanding of data.\n",
      "\n",
      "3. **Elasticsearch**: This is an open-source, distributed search and analytics engine. It centrally stores your data for search, analytics and use in machine learning. In this case, it collects and stores logs from our application's microservices via Fluentd or Logstash.\n",
      "\n",
      "4. **Kibana**: This is a data visualization dashboard for Elasticsearch. It provides visualization capabilities on top of the content indexed in Elasticsearch. With Kibana, we can have customized dashboards for different log data to monitor the system in real time, debug issues, and analyze historical data.\n",
      "\n",
      "5. **Amazon S3 or Google Cloud Storage (Optional)**: In high-data environments, storing all logs over time might overwhelm even a well-configured Elasticsearch cluster. Here, long-term storage options like S3 or GCP can be used, where logs beyond a certain age can be migrated for compliance or rare debug scenarios. \n",
      "\n",
      "6. **Apache Kafka (Optional)**: For systems with very high log throughput, Apache Kafka can be added as a buffer between log shippers and Elasticsearch. This ensures the robust delivery of logs under heavy load and improves the system's fault tolerance.\n",
      "\n",
      "----------------------------------------------------------------------------\n",
      "\n",
      "This architecture can entertain logging needs of different scales and requirements while providing real-time operational insights via Kibana's rich visualization capabilities.\n",
      "\n",
      "### Layer 2: Component Details ###\n",
      "\n",
      "1. **Logging Agent Running on Each Microservice**\n",
      "The Logging Agent (like Fluentd or Logstash) is a lightweight process that can be deployed on the same host where the microservice is running. The primary purpose of the logging agent is to monitor the log files generated by the microservice, collect new logs, convert them to a standard format (if necessary), and ship them to the log aggregation system (e.g., Elasticsearch, Kafka).\n",
      "\n",
      "The agent can also provide additional features: \n",
      "- **Log Filtering**: Discard the non-significant logs at the source, or filter based on the severity (e.g., debug, info, warn, error).\n",
      "- **Field Extraction and Transformation**: Extract structured data from log entries to provide better context about the logged event. For example, in case if logs are in text format, it can convert them to JSON.\n",
      "- **Performance Tuning**: It can buffer logs and send them in batches, which improves network efficiency.\n",
      "\n",
      "2. **Message Queue's Configuration**\n",
      "A distributed message queue like Apache Kafka can be used in this architecture. The primary role of Kafka is to serve as a buffer for collected logs, which guards against data loss during peak load, outages, or network issues.\n",
      "\n",
      "Let's take a look at these configuration concepts:\n",
      "\n",
      "- **Partitioning**: Kafka topics are divided into partitions, which can be distributed over many servers for horizontal scalability. Logs can be mapped to partitions in many ways, e.g., round-robin, or based on some key (like log source) to ensure all logs from a particular source always end up in the same partition for order consistency.\n",
      "\n",
      "- **Replication**: Kafka's replication mechanism ensures the durability and fault tolerance of log data. Each partition can have multiple replicas, distributed on different brokers. If a broker dies, replicas on other brokers ensure no data loss.\n",
      "\n",
      "- **Consumer Group**: Log aggregation system (like Elasticsearch) would be a consumer. In a large setup, you could have multiple instances of Elasticsearch grouped together to consume logs from Kafka concurrently. If one consumer fails, Kafka rebalances logs across remaining consumers automatically.\n",
      "\n",
      "3. **Log Indexing and Query Optimization**\n",
      "Logs are structured and stored in an indexed format in Elasticsearch which provides extremely fast search capabilities even across large volumes of data. By creating a proper index (like time-series index), the system helps in searching, analyzing and querying the logs effeciently and quickly.\n",
      "\n",
      "- **Sharding**: Elasticsearch splits the index into multiple pieces called shards, which allows the distribution of data in such a way that it can be stored on multiple nodes, aiding horizontal scalability.\n",
      "  \n",
      "- **Index Rotation**: It’s common to configure Elasticsearch to use an index-per-day pattern for logs. It provides two significant benefits: \n",
      "    - Smoother Performance: Having daily indices helps distribute the write load, as new logs always write into a new index. \n",
      "    - Easy Old data cleanup: As each index corresponds to a specific day, deleting old data as per retention policy is as simple and efficient as dropping an old index.\n",
      "\n",
      "- **Replication**: Each shard in Elasticsearch can have zero or more replicas for better reliability and higher read throughput. Read queries can be executed on any replica, balancing the load.\n",
      "\n",
      "- **Document Mapping**: An optimal Elasticsearch document schema (or mapping) can further optimize log storage and search. The schema should align with the common queries on logs.\n",
      " \n",
      "- **Result Caching**: Elasticsearch automatically caches frequently used filters and aggregations, providing very fast responses for repeated queries.\n",
      "\n",
      "All these factors together work towards ensuring the log indexing and querying processes are optimized, leading to better log management and improved system monitoring and diagnosis.\n",
      "\n",
      "### Layer 3: Operational Considerations ###\n",
      "\n",
      "**1. Fault Tolerance**\n",
      "\n",
      "Fault tolerance in our system can be achieved at different levels:\n",
      "\n",
      "- **Microservice Level**: Microservices should implement a retry mechanism to handle the sending of logs to the logging agent in case of temporary failures.\n",
      "\n",
      "- **Logging Agent Level**: Logging agents should implement a backpressure mechanism to adjust the rate of incoming logs according to its own speed and the speed of the log aggregation system.\n",
      "\n",
      "- **Kafka Level**: Kafka provides fault tolerance features like message replication across multiple brokers and automatic failover.\n",
      "\n",
      "- **Elasticsearch Level**: Elasticsearch ensures fault-tolerance through sharding and replication. If one node fails, another can serve the request.\n",
      "\n",
      "**2. Performance Optimization**\n",
      "\n",
      "- **Batching**: Both the logging agents and Kafka can buffer and batch messages. Batching minimizes the number of network calls, thus saving CPU and network resources.\n",
      "\n",
      "- **Compression**: Kafka supports message compression which can reduce I/O operations and network traffic.\n",
      "\n",
      "- **Load Balancing**: Using a load balancer in front of Elasticsearch nodes ensures even distribution of read and write traffic. Also, read requests can be served by any replica increasing read throughput.\n",
      "\n",
      "- **Indexing Strategies**: Leverage Elasticsearch's ability to perform indexing asynchronously. Also, consider indexing only the necessary fields to keep the size of indices small and make searches faster.\n",
      "\n",
      "**3. Scalability**\n",
      "\n",
      "- **Microservice Level**: Increase the number of microservice instances as the application grows. The number of logging agents will scale in proportion as they are deployed on the same hosts.\n",
      "\n",
      "- **Kafka Level**: As the volume of logs increases, we can add more partitions to the Kafka topic or add more brokers and distribute partitions across them.\n",
      "\n",
      "- **Elasticsearch Level**: Scaling can be achieved by adding more nodes to the Elasticsearch cluster, and data can be rebalanced across old and new nodes. If hot-warm architecture is employed, new indices can be created in the hot nodes to provide high-speed write capability, and older indices can be moved to warm nodes where they are read more frequently than written.\n",
      "\n",
      "Implementing these operational considerations will help us create a scalable and fault-tolerant logging system with optimized performance.\n"
     ]
    }
   ],
   "source": [
    "def layered_contextual_prompting():\n",
    "    # Layer 1: Define the high-level architecture\n",
    "    layer1_prompt = \"\"\"\n",
    "You are a senior software architect. Your task is to design a high-level logging system \n",
    "for a distributed application.\n",
    "\n",
    "Context:\n",
    "- The system is deployed across multiple regions with hundreds of microservices.\n",
    "- Logs need to be collected, stored, and queried efficiently.\n",
    "\n",
    "Output:\n",
    "1. A high-level architecture diagram in textual form.\n",
    "2. Technologies involved and their roles.\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate the response for Layer 1\n",
    "    layer1_response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a software architecture expert.\"},\n",
    "            {\"role\": \"user\", \"content\": layer1_prompt},\n",
    "        ],\n",
    "        stream=False\n",
    "    ).choices[0].message.content\n",
    "\n",
    "    print(\"\\n### Layer 1: High-Level Architecture ###\\n\")\n",
    "    print(layer1_response)\n",
    "\n",
    "    # Layer 2: Drill down into component-level details\n",
    "    layer2_prompt = f\"\"\"\n",
    "You have designed the high-level logging architecture below:\n",
    "{layer1_response}\n",
    "\n",
    "Next Step:\n",
    "Provide a detailed breakdown of the components. Specifically:\n",
    "1. The role of the logging agent running on each microservice.\n",
    "2. The message queue's configuration (e.g., partitioning, replication).\n",
    "3. How log indexing and querying are optimized.\n",
    "    \"\"\"\n",
    "\n",
    "    layer2_response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a distributed systems expert.\"},\n",
    "            {\"role\": \"user\", \"content\": layer2_prompt},\n",
    "        ],\n",
    "        stream=False\n",
    "    ).choices[0].message.content\n",
    "\n",
    "    print(\"\\n### Layer 2: Component Details ###\\n\")\n",
    "    print(layer2_response)\n",
    "\n",
    "    # Layer 3: Operational considerations\n",
    "    layer3_prompt = f\"\"\"\n",
    "You have detailed the component-level design:\n",
    "{layer2_response}\n",
    "\n",
    "Next Step:\n",
    "Address the following operational considerations:\n",
    "1. Fault tolerance: How the system handles component failures.\n",
    "2. Performance optimization: Minimizing latency in log collection and querying.\n",
    "3. Scalability: Ensuring the system can scale as the application grows.\n",
    "    \"\"\"\n",
    "\n",
    "    layer3_response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert in scalable and fault-tolerant systems.\"},\n",
    "            {\"role\": \"user\", \"content\": layer3_prompt},\n",
    "        ],\n",
    "        stream=False\n",
    "    ).choices[0].message.content\n",
    "\n",
    "    print(\"\\n### Layer 3: Operational Considerations ###\\n\")\n",
    "    print(layer3_response)\n",
    "\n",
    "# Execute the function\n",
    "layered_contextual_prompting()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c30d7d-1707-4f4a-ad6d-2ed14295a28c",
   "metadata": {},
   "source": [
    "## 15. Zero-Shot Task Completion with Meta-Instructions\n",
    "### Concept: Rely on advanced instructions to perform completely novel tasks without examples.\n",
    "### Use Case: Generating novel ideas or tasks with no prior training or examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0efbaff-d45c-48ea-944c-546d7568f441",
   "metadata": {},
   "source": [
    "## 17. Tree of Thought (ToT)\n",
    "### Concept: A systematic exploration of potential solutions for tasks using a tree structure, leveraging strategic planning and pruning techniques (e.g., breadth-first search).\n",
    "### Use Case: Complex problem-solving and exploration tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d15aac-0cb0-49ac-97d6-08c8eeb7e3c0",
   "metadata": {},
   "source": [
    "## 19. Uncertainty-Based Prompting\n",
    "### Concept: Encourages the model to evaluate its confidence in outputs and flag uncertain responses for human review or re-verification.\n",
    "### Use Case: Tasks requiring reliability or risk evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd6cf60-a13c-41cd-bff5-914963cc2f9d",
   "metadata": {},
   "source": [
    "## 16. ReAct Prompting (Reasoning + Acting)\n",
    "### Concept: Combines reasoning (chain of thought) with actionable steps to interact with environments or tools. The model alternates between reasoning through a problem and executing actions.\n",
    "### Use Case: Generating novel ideas or tasks with no prior training or examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17bea65e-f820-4031-a66b-6d6c8a6e5028",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "create_react_agent() got an unexpected keyword argument 'template'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 57\u001b[0m\n\u001b[1;32m     51\u001b[0m tools \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     52\u001b[0m     square_two_digits_tool,\n\u001b[1;32m     53\u001b[0m     closest_perfect_square_three_digits_tool,\n\u001b[1;32m     54\u001b[0m ]\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Create the ReAct agent using the hwchase17/react-chat template\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m agent \u001b[38;5;241m=\u001b[39m create_react_agent(\n\u001b[1;32m     58\u001b[0m     tools\u001b[38;5;241m=\u001b[39mtools,\n\u001b[1;32m     59\u001b[0m     llm\u001b[38;5;241m=\u001b[39mllm,\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m# verbose=True,\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     template\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhwchase17/react-chat\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Specify the react-chat template\u001b[39;00m\n\u001b[1;32m     63\u001b[0m )\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Define the task that the agent should perform\u001b[39;00m\n\u001b[1;32m     66\u001b[0m task \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFind the square of 23. Then find the closest perfect square for 289.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: create_react_agent() got an unexpected keyword argument 'template'"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import Tool, create_react_agent\n",
    "from langchain.agents.agent_types import AgentType\n",
    "import math\n",
    "\n",
    "# Define tools with explicit name and description\n",
    "def square_two_digits(input: str) -> str:\n",
    "    \"\"\"\n",
    "    Calculates the square of a two-digit number.\n",
    "    The input must be a valid two-digit integer between 10 and 99.\n",
    "    \"\"\"\n",
    "    number = int(input)\n",
    "    if 10 <= number < 100:\n",
    "        return f\"The square of {number} is {number ** 2}.\"\n",
    "    return \"Input is not a valid two-digit number.\"\n",
    "\n",
    "# Define the tool for calculating the square of two digits\n",
    "square_two_digits_tool = Tool.from_function(\n",
    "    func=square_two_digits,\n",
    "    name=\"Square Two Digits\",\n",
    "    description=\"This tool calculates the square of a two-digit number. The input must be a valid two-digit integer between 10 and 99.\"\n",
    ")\n",
    "\n",
    "def closest_perfect_square_three_digits(input: str) -> str:\n",
    "    \"\"\"\n",
    "    Finds the closest perfect square for a three-digit number.\n",
    "    Returns the square root and the square value.\n",
    "    \"\"\"\n",
    "    number = int(input)\n",
    "    if 100 <= number < 1000:\n",
    "        closest_root = round(math.sqrt(number))\n",
    "        closest_square = closest_root ** 2\n",
    "        return f\"Closest perfect square to {number} is {closest_square} with square root {closest_root}.\"\n",
    "    return \"Input is not a valid three-digit number.\"\n",
    "\n",
    "# Define the tool for finding the closest perfect square\n",
    "closest_perfect_square_three_digits_tool = Tool.from_function(\n",
    "    func=closest_perfect_square_three_digits,\n",
    "    name=\"Closest Perfect Square for 3 Digits\",\n",
    "    description=\"This tool finds the closest perfect square for a three-digit number. The input must be a valid three-digit integer between 100 and 999.\"\n",
    ")\n",
    "\n",
    "# Instantiate the LLM (Large Language Model)\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4\",\n",
    "    temperature=0,\n",
    "    openai_api_key=\"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\"\n",
    ")\n",
    "\n",
    "# Define the tools to be used in the agent\n",
    "tools = [\n",
    "    square_two_digits_tool,\n",
    "    closest_perfect_square_three_digits_tool,\n",
    "]\n",
    "\n",
    "# Create the ReAct agent using the hwchase17/react-chat template\n",
    "agent = create_react_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    # agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    # verbose=True,\n",
    "    template=\"hwchase17/react-chat\"  # Specify the react-chat template\n",
    ")\n",
    "\n",
    "# Define the task that the agent should perform\n",
    "task = \"Find the square of 23. Then find the closest perfect square for 289.\"\n",
    "\n",
    "# Run the agent to solve the task\n",
    "response = agent.run(input=task)\n",
    "\n",
    "# Output the response\n",
    "print(\"\\n### ReAct-Chat Agent Response ###\\n\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3c78343a-3ca5-4116-8c20-6c27c15be146",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_react_agent, create_openai_tools_agent\n",
    "from langchain_openai import OpenAI, ChatOpenAI\n",
    "from langchain_core.tools import tool\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "95660e2b-0af8-4f59-bf6b-851c4574b970",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def return_two_digits(input: str) -> str:\n",
    "    \"\"\"\n",
    "    The input valid integer between 100 and 999.\n",
    "    Returns the last 2 digits.\n",
    "    \"\"\"\n",
    "    number = int(input) % 100\n",
    "    return f\"The square of {number} is {number ** 2}.\"\n",
    "\n",
    "@tool\n",
    "def square_two_digits(input: str) -> str:\n",
    "    \"\"\"\n",
    "    The input valid 2 digit integer between 10 and 99.\n",
    "    Return the square.\n",
    "    \"\"\"\n",
    "    number = int(input)\n",
    "    return f\"The square of {number} is {number ** 2}.\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def closest_perfect_square_three_digits(input: str) -> str:\n",
    "    \"\"\"\n",
    "    Finds the closest perfect square for a three-digit number.\n",
    "    Returns the square root and the square value.\n",
    "    \"\"\"\n",
    "    number = int(input)\n",
    "    if 100 <= number < 1000:\n",
    "        closest_root = round(math.sqrt(number))\n",
    "        closest_square = closest_root ** 2\n",
    "        return f\"Closest perfect square to {number} is {closest_square} with square root {closest_root}.\"\n",
    "    return \"Input is not a valid three-digit number.\"\n",
    "\n",
    "tools = [square_two_digits, closest_perfect_square_three_digits, return_two_digits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "139d952f-d3bd-4bc2-a3a7-4d8668e767f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/promptingtechnique/lib/python3.12/site-packages/langsmith/client.py:241: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "llm1 = ChatOpenAI(model=\"gpt-4\", api_key=\"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\")\n",
    "\n",
    "# Construct the OpenAI Tools agent\n",
    "agent1 = create_openai_tools_agent(llm1, tools, prompt)\n",
    "\n",
    "llm2 = ChatOpenAI(model=\"gpt-4\", api_key=\"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\")\n",
    "prompt2 = hub.pull(\"hwchase17/react\")\n",
    "agent2 = create_react_agent(llm2, tools, prompt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8303e84a-6819-48e1-8bdc-52fbaa58b264",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"I give you the number 124. You can call multiple tools that you have.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9661bd9c-f161-41a0-94d1-030ecb2df6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an agent executor by passing in the agent and tools\n",
    "agent_executor = AgentExecutor(agent=agent1, tools=tools, verbose=True, return_intermediate_steps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "06688e19-74ba-4d4a-8f48-3cd593906c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor2 = AgentExecutor(agent=agent2, tools=tools, verbose=True, return_intermediate_steps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ffa47708-d6a8-4cfe-bbeb-efea55c95f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `square_two_digits` with `{'input': '124'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mThe square of 124 is 15376.\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `closest_perfect_square_three_digits` with `{'input': '124'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mClosest perfect square to 124 is 121 with square root 11.\u001b[0m\u001b[32;1m\u001b[1;3mFor the number 124, if we consider the last two digits which are 24, the square of 24 is 576. If we consider the whole number 124, the closest perfect square is 121, and its square root is 11.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'I give you the number 124. You can call multiple tools that you have.',\n",
       " 'output': 'For the number 124, if we consider the last two digits which are 24, the square of 24 is 576. If we consider the whole number 124, the closest perfect square is 121, and its square root is 11.',\n",
       " 'intermediate_steps': [(ToolAgentAction(tool='square_two_digits', tool_input={'input': '124'}, log=\"\\nInvoking: `square_two_digits` with `{'input': '124'}`\\n\\n\\n\", message_log=[AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_1UfWlSB9Mp9f0QPqjxF6Ng30', 'function': {'arguments': '{\\n\"input\": \"124\"\\n}', 'name': 'square_two_digits'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4-0613'}, id='run-ca0134ad-9180-48c5-92b0-6bcd58cc057c', tool_calls=[{'name': 'square_two_digits', 'args': {'input': '124'}, 'id': 'call_1UfWlSB9Mp9f0QPqjxF6Ng30', 'type': 'tool_call'}], tool_call_chunks=[{'name': 'square_two_digits', 'args': '{\\n\"input\": \"124\"\\n}', 'id': 'call_1UfWlSB9Mp9f0QPqjxF6Ng30', 'index': 0, 'type': 'tool_call_chunk'}])], tool_call_id='call_1UfWlSB9Mp9f0QPqjxF6Ng30'),\n",
       "   'The square of 124 is 15376.'),\n",
       "  (ToolAgentAction(tool='closest_perfect_square_three_digits', tool_input={'input': '124'}, log=\"\\nInvoking: `closest_perfect_square_three_digits` with `{'input': '124'}`\\n\\n\\n\", message_log=[AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_bd6gGsI2qsfnTuvJHoAyGOWb', 'function': {'arguments': '{\"input\": \"124\"}', 'name': 'closest_perfect_square_three_digits'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4-0613'}, id='run-848cab61-49ac-40fc-b90a-006ec49e0397', tool_calls=[{'name': 'closest_perfect_square_three_digits', 'args': {'input': '124'}, 'id': 'call_bd6gGsI2qsfnTuvJHoAyGOWb', 'type': 'tool_call'}], tool_call_chunks=[{'name': 'closest_perfect_square_three_digits', 'args': '{\"input\": \"124\"}', 'id': 'call_bd6gGsI2qsfnTuvJHoAyGOWb', 'index': 0, 'type': 'tool_call_chunk'}])], tool_call_id='call_bd6gGsI2qsfnTuvJHoAyGOWb'),\n",
       "   'Closest perfect square to 124 is 121 with square root 11.')]}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c21d4c31-83b4-42b0-937f-794f09a53284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThe number is a three digit number. I can either find the closest perfect square for this number or return the last 2 digits of this number.\n",
      "Action: return_two_digits\n",
      "Action Input: 124\u001b[0m\u001b[38;5;200m\u001b[1;3mThe square of 24 is 576.\u001b[0m\u001b[32;1m\u001b[1;3mNow I know the last two digits and its square.\n",
      "Action: square_two_digits\n",
      "Action Input: 24\u001b[0m\u001b[36;1m\u001b[1;3mThe square of 24 is 576.\u001b[0m\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: 576\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'I give you the number 124. You can call multiple tools that you have.',\n",
       " 'output': '576',\n",
       " 'intermediate_steps': [(AgentAction(tool='return_two_digits', tool_input='124', log='The number is a three digit number. I can either find the closest perfect square for this number or return the last 2 digits of this number.\\nAction: return_two_digits\\nAction Input: 124'),\n",
       "   'The square of 24 is 576.'),\n",
       "  (AgentAction(tool='square_two_digits', tool_input='24', log='Now I know the last two digits and its square.\\nAction: square_two_digits\\nAction Input: 24'),\n",
       "   'The square of 24 is 576.')]}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor2.invoke({\"input\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "99af3492-961d-405a-b7f5-3290c5170432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aiohappyeyeballs==2.4.3\n",
      "aiohttp==3.11.7\n",
      "aiosignal==1.3.1\n",
      "annotated-types==0.7.0\n",
      "anyio @ file:///private/var/folders/c_/qfmhj66j0tn016nkx_th4hxm0000gp/T/abs_d3wggtkt2s/croot/anyio_1729121282950/work\n",
      "appnope @ file:///Users/builder/cbouss/perseverance-python-buildout/croot/appnope_1699320997592/work\n",
      "argon2-cffi @ file:///opt/conda/conda-bld/argon2-cffi_1645000214183/work\n",
      "argon2-cffi-bindings @ file:///Users/builder/cbouss/perseverance-python-buildout/croot/argon2-cffi-bindings_1699264521425/work\n",
      "asttokens @ file:///opt/conda/conda-bld/asttokens_1646925590279/work\n",
      "async-lru @ file:///Users/builder/cbouss/perseverance-python-buildout/croot/async-lru_1701806219228/work\n",
      "attrs @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_70ri_4okcz/croot/attrs_1729089428961/work\n",
      "Babel @ file:///Users/builder/cbouss/perseverance-python-buildout/croot/babel_1699247663878/work\n",
      "beautifulsoup4 @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_74rj45rt1p/croot/beautifulsoup4-split_1718029835783/work\n",
      "bleach @ file:///opt/conda/conda-bld/bleach_1641577558959/work\n",
      "Brotli @ file:///private/var/folders/c_/qfmhj66j0tn016nkx_th4hxm0000gp/T/abs_67j4p18ie3/croot/brotli-split_1714483158198/work\n",
      "certifi @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_c7dh9wvd8f/croot/certifi_1725551679893/work/certifi\n",
      "cffi @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_a1n4t3qsjy/croot/cffi_1726856449083/work\n",
      "charset-normalizer @ file:///croot/charset-normalizer_1721748349566/work\n",
      "comm @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_aef2nlxc2o/croot/comm_1709322866518/work\n",
      "dataclasses-json==0.6.7\n",
      "debugpy @ file:///Users/builder/cbouss/perseverance-python-buildout/croot/debugpy_1699267934478/work\n",
      "decorator @ file:///opt/conda/conda-bld/decorator_1643638310831/work\n",
      "defusedxml @ file:///tmp/build/80754af9/defusedxml_1615228127516/work\n",
      "distro==1.9.0\n",
      "executing @ file:///opt/conda/conda-bld/executing_1646925071911/work\n",
      "fastjsonschema @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_13il3azn5x/croot/python-fastjsonschema_1731939499843/work\n",
      "frozenlist==1.5.0\n",
      "greenlet==3.1.1\n",
      "h11 @ file:///private/var/folders/c_/qfmhj66j0tn016nkx_th4hxm0000gp/T/abs_14h6x3q_k6/croot/h11_1706652288187/work\n",
      "httpcore @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_2ek8c13vuv/croot/httpcore_1706728466947/work\n",
      "httpx @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_74uqix89pr/croot/httpx_1723474816711/work\n",
      "httpx-sse==0.4.0\n",
      "idna @ file:///private/var/folders/c_/qfmhj66j0tn016nkx_th4hxm0000gp/T/abs_2b_jn555_n/croot/idna_1714398852258/work\n",
      "ipykernel @ file:///private/var/folders/c_/qfmhj66j0tn016nkx_th4hxm0000gp/T/abs_26cab5zq0o/croot/ipykernel_1728665600114/work\n",
      "ipython @ file:///private/var/folders/c_/qfmhj66j0tn016nkx_th4hxm0000gp/T/abs_87xjefcpgk/croot/ipython_1726064241225/work\n",
      "jedi @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_dc59flyfgj/croot/jedi_1721058355050/work\n",
      "Jinja2 @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_8dveqartkv/croot/jinja2_1730902844110/work\n",
      "jiter==0.7.1\n",
      "json5 @ file:///private/var/folders/c_/qfmhj66j0tn016nkx_th4hxm0000gp/T/abs_f7m7nb46i6/croot/json5_1730786807490/work\n",
      "jsonpatch==1.33\n",
      "jsonpointer==3.0.0\n",
      "jsonschema @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_20pyq8pymy/croot/jsonschema_1728486714853/work\n",
      "jsonschema-specifications @ file:///Users/builder/cbouss/perseverance-python-buildout/croot/jsonschema-specifications_1701805291791/work\n",
      "jupyter-events @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_0ahkeborfn/croot/jupyter_events_1718738118710/work\n",
      "jupyter-lsp @ file:///Users/builder/cbouss/perseverance-python-buildout/croot/jupyter-lsp-meta_1707339505175/work\n",
      "jupyter_client @ file:///Users/builder/cbouss/perseverance-python-buildout/croot/jupyter_client_1701805419641/work\n",
      "jupyter_core @ file:///private/var/folders/c_/qfmhj66j0tn016nkx_th4hxm0000gp/T/abs_80l8bw91xp/croot/jupyter_core_1718818299834/work\n",
      "jupyter_server @ file:///private/var/folders/c_/qfmhj66j0tn016nkx_th4hxm0000gp/T/abs_d76py1hq6i/croot/jupyter_server_1718827088455/work\n",
      "jupyter_server_terminals @ file:///Users/builder/cbouss/perseverance-python-buildout/croot/jupyter_server_terminals_1701805810278/work\n",
      "jupyterlab @ file:///private/var/folders/c_/qfmhj66j0tn016nkx_th4hxm0000gp/T/abs_9376d80062/croot/jupyterlab_1725895220583/work\n",
      "jupyterlab-pygments @ file:///tmp/build/80754af9/jupyterlab_pygments_1601490720602/work\n",
      "jupyterlab_server @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_3ak8oma7hk/croot/jupyterlab_server_1725865365518/work\n",
      "langchain==0.3.8\n",
      "langchain-community==0.3.8\n",
      "langchain-core==0.3.21\n",
      "langchain-openai==0.2.9\n",
      "langchain-text-splitters==0.3.2\n",
      "langsmith==0.1.145\n",
      "MarkupSafe @ file:///Users/builder/cbouss/perseverance-python-buildout/croot/markupsafe_1707339363573/work\n",
      "marshmallow==3.23.1\n",
      "matplotlib-inline @ file:///Users/builder/cbouss/perseverance-python-buildout/croot/matplotlib-inline_1699248719910/work\n",
      "mistune @ file:///Users/builder/cbouss/perseverance-python-buildout/croot/mistune_1699277668306/work\n",
      "multidict==6.1.0\n",
      "mypy-extensions==1.0.0\n",
      "nb_conda_kernels @ file:///usr/share/miniconda/conda-bld/nb_conda_kernels_1730743802974/work\n",
      "nbclient @ file:///Users/builder/cbouss/perseverance-python-buildout/croot/nbclient_1701805585792/work\n",
      "nbconvert @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_2eojbpwhtf/croot/nbconvert_1728049972177/work\n",
      "nbformat @ file:///private/var/folders/c_/qfmhj66j0tn016nkx_th4hxm0000gp/T/abs_a83923jn5m/croot/nbformat_1728049884447/work\n",
      "nest-asyncio @ file:///private/var/folders/c_/qfmhj66j0tn016nkx_th4hxm0000gp/T/abs_c44_atq5c1/croot/nest-asyncio_1708532677863/work\n",
      "notebook @ file:///private/var/folders/c_/qfmhj66j0tn016nkx_th4hxm0000gp/T/abs_503eqqjohb/croot/notebook_1727197361613/work\n",
      "notebook_shim @ file:///Users/builder/cbouss/perseverance-python-buildout/croot/notebook-shim_1707339593183/work\n",
      "numpy==1.26.4\n",
      "openai==1.55.0\n",
      "orjson==3.10.12\n",
      "overrides @ file:///Users/builder/cbouss/perseverance-python-buildout/croot/overrides_1701805937714/work\n",
      "packaging @ file:///private/var/folders/c_/qfmhj66j0tn016nkx_th4hxm0000gp/T/abs_af6w2d9k11/croot/packaging_1720101860461/work\n",
      "pandocfilters @ file:///opt/conda/conda-bld/pandocfilters_1643405455980/work\n",
      "parso @ file:///opt/conda/conda-bld/parso_1641458642106/work\n",
      "pexpect @ file:///tmp/build/80754af9/pexpect_1605563209008/work\n",
      "platformdirs @ file:///Users/builder/cbouss/perseverance-python-buildout/croot/platformdirs_1701805067573/work\n",
      "prometheus_client @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_6ee4g8mmv4/croot/prometheus_client_1731958834227/work\n",
      "prompt-toolkit @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_4d8kk9w3ed/croot/prompt-toolkit_1704404354789/work\n",
      "propcache==0.2.0\n",
      "psutil @ file:///Users/builder/cbouss/perseverance-python-buildout/croot/psutil_1699246686105/work\n",
      "ptyprocess @ file:///tmp/build/80754af9/ptyprocess_1609355006118/work/dist/ptyprocess-0.7.0-py2.py3-none-any.whl\n",
      "pure-eval @ file:///opt/conda/conda-bld/pure_eval_1646925070566/work\n",
      "pycparser @ file:///tmp/build/80754af9/pycparser_1636541352034/work\n",
      "pydantic==2.10.1\n",
      "pydantic-settings==2.6.1\n",
      "pydantic_core==2.27.1\n",
      "Pygments @ file:///Users/builder/cbouss/perseverance-python-buildout/croot/pygments_1699240212223/work\n",
      "PySocks @ file:///Users/builder/cbouss/perseverance-python-buildout/croot/pysocks_1699239289103/work\n",
      "python-dateutil @ file:///private/var/folders/c_/qfmhj66j0tn016nkx_th4hxm0000gp/T/abs_efk5_uakg8/croot/python-dateutil_1716495742183/work\n",
      "python-dotenv==1.0.1\n",
      "python-json-logger @ file:///Users/builder/cbouss/perseverance-python-buildout/croot/python-json-logger_1699260970759/work\n",
      "pytz @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_244ln7biq9/croot/pytz_1713974320727/work\n",
      "PyYAML @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_34t43scp15/croot/pyyaml_1728658999838/work\n",
      "pyzmq @ file:///private/var/folders/c_/qfmhj66j0tn016nkx_th4hxm0000gp/T/abs_c7mhasdker/croot/pyzmq_1705605099593/work\n",
      "referencing @ file:///Users/builder/cbouss/perseverance-python-buildout/croot/referencing_1701805251345/work\n",
      "regex==2024.11.6\n",
      "requests @ file:///private/var/folders/c_/qfmhj66j0tn016nkx_th4hxm0000gp/T/abs_2eh84h3yiv/croot/requests_1730999127987/work\n",
      "requests-toolbelt==1.0.0\n",
      "rfc3339-validator @ file:///Users/builder/cbouss/perseverance-python-buildout/croot/rfc3339-validator_1699261389119/work\n",
      "rfc3986-validator @ file:///Users/builder/cbouss/perseverance-python-buildout/croot/rfc3986-validator_1699261424790/work\n",
      "rpds-py @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_993n95v2oo/croot/rpds-py_1732230862301/work\n",
      "Send2Trash @ file:///Users/builder/cbouss/perseverance-python-buildout/croot/send2trash_1701806045227/work\n",
      "setuptools==75.1.0\n",
      "six @ file:///tmp/build/80754af9/six_1644875935023/work\n",
      "sniffio @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_a46njf828g/croot/sniffio_1705431298111/work\n",
      "soupsieve @ file:///Users/builder/cbouss/perseverance-python-buildout/croot/soupsieve_1699283107629/work\n",
      "SQLAlchemy==2.0.35\n",
      "stack-data @ file:///opt/conda/conda-bld/stack_data_1646927590127/work\n",
      "tenacity==9.0.0\n",
      "terminado @ file:///Users/builder/cbouss/perseverance-python-buildout/croot/terminado_1699284203253/work\n",
      "tiktoken==0.8.0\n",
      "tinycss2 @ file:///Users/builder/cbouss/perseverance-python-buildout/croot/tinycss2_1699262458881/work\n",
      "tornado @ file:///private/var/folders/c_/qfmhj66j0tn016nkx_th4hxm0000gp/T/abs_7e0ruwlfee/croot/tornado_1718740118099/work\n",
      "tqdm==4.67.0\n",
      "traitlets @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_f49losghmg/croot/traitlets_1718227069025/work\n",
      "typing-inspect==0.9.0\n",
      "typing_extensions==4.12.2\n",
      "urllib3 @ file:///private/var/folders/c_/qfmhj66j0tn016nkx_th4hxm0000gp/T/abs_7dvsr0a5y0/croot/urllib3_1727769823716/work\n",
      "wcwidth @ file:///Users/ktietz/demo/mc3/conda-bld/wcwidth_1629357192024/work\n",
      "webencodings @ file:///Users/builder/cbouss/perseverance-python-buildout/croot/webencodings_1699250987289/work\n",
      "websocket-client @ file:///private/var/folders/c_/qfmhj66j0tn016nkx_th4hxm0000gp/T/abs_8c3cjp8r36/croot/websocket-client_1715878317325/work\n",
      "wheel==0.44.0\n",
      "yarl==1.18.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip freeze\n",
    "\n",
    "# \"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\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:promptingtechnique] *",
   "language": "python",
   "name": "conda-env-promptingtechnique-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
